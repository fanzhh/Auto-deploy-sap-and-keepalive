{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1sIeSUrdBCXqhM72eEnnX9T4Bw6mojIbM",
      "authorship_tag": "ABX9TyMoUX9r4N6UOpprOoga1z7Y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fanzhh/Auto-deploy-sap-and-keepalive/blob/main/preprocess_gmp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GK3_DFhj4NOs",
        "outputId": "9d44ea1c-aac7-4f22-dedb-926f75864302"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The capital of France is **Paris**.\n"
          ]
        }
      ],
      "source": [
        "from google.colab import ai\n",
        "\n",
        "response = ai.generate_text(\"What is the capital of France?\")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    from google.colab import ai\n",
        "    COLAB_AVAILABLE = True\n",
        "    print(\"âœ“ Google Colab AIå·²åŠ è½½\")\n",
        "except ImportError:\n",
        "    COLAB_AVAILABLE = False\n",
        "    print(\"âš  æœªåœ¨Colabç¯å¢ƒä¸­ï¼Œå°†ä½¿ç”¨å¤‡ç”¨é…ç½®\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KEnS5-1j7xaN",
        "outputId": "a86722c4-bd1c-448d-a670-4e8bca823d8d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ“ Google Colab AIå·²åŠ è½½\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import time\n",
        "import logging\n",
        "import re\n",
        "from pathlib import Path\n",
        "from typing import List, Dict, Optional\n",
        "from dataclasses import dataclass\n",
        "import threading\n",
        "from datetime import datetime\n",
        "import requests\n",
        "\n",
        "# Colabç¯å¢ƒé…ç½®\n",
        "class ColabConfig:\n",
        "    \"\"\"Colabç¯å¢ƒé…ç½®ç±»\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.colab_available = COLAB_AVAILABLE\n",
        "        # ä½¿ç”¨å½“å‰ç›®å½•è€Œä¸æ˜¯/content\n",
        "        self.working_dir = os.getcwd()\n",
        "        self.gmps_dir = f\"{self.working_dir}/processed\"  # è¯»å–processedç›®å½•ä¸­çš„æ–‡ä»¶\n",
        "        self.jsons_dir = f\"{self.working_dir}/jsons\"     # è¾“å‡ºåˆ°jsonsç›®å½•\n",
        "        self.processed_dir = f\"{self.working_dir}/processed\"  # processedç›®å½•\n",
        "\n",
        "    def setup_directories(self):\n",
        "        \"\"\"åˆ›å»ºå¿…è¦çš„å·¥ä½œç›®å½•\"\"\"\n",
        "        for dir_path in [self.jsons_dir]:\n",
        "            if not os.path.exists(dir_path):\n",
        "                os.makedirs(dir_path, exist_ok=True)\n",
        "                print(f\"âœ“ åˆ›å»ºç›®å½•: {dir_path}\")\n",
        "\n",
        "        # æ£€æŸ¥processedç›®å½•\n",
        "        if not os.path.exists(self.processed_dir):\n",
        "            print(f\"âš ï¸ è­¦å‘Š: processedç›®å½•ä¸å­˜åœ¨: {self.processed_dir}\")\n",
        "        else:\n",
        "            print(f\"âœ“ æ‰¾åˆ°processedç›®å½•: {self.processed_dir}\")\n",
        "\n",
        "        # æ£€æŸ¥jsonsç›®å½•\n",
        "        if os.path.exists(self.jsons_dir):\n",
        "            print(f\"âœ“ æ‰¾åˆ°jsonsç›®å½•: {self.jsons_dir}\")\n",
        "\n",
        "        print(f\"âœ“ å½“å‰å·¥ä½œç›®å½•: {self.working_dir}\")\n",
        "\n",
        "    def get_upload_instructions(self):\n",
        "        \"\"\"è·å–æ–‡ä»¶å¤„ç†è¯´æ˜\"\"\"\n",
        "        return f\"\"\"\n",
        "æ–‡ä»¶å¤„ç†è¯´æ˜ï¼š\n",
        "\n",
        "1. è¾“å…¥æ–‡ä»¶: ä» processed/ ç›®å½•è¯»å–.txtæ–‡ä»¶\n",
        "2. è¾“å‡ºæ–‡ä»¶: ç”Ÿæˆçš„JSONæ–‡ä»¶ä¿å­˜åˆ° jsons/ ç›®å½•\n",
        "\n",
        "å½“å‰ç›®å½•ç»“æ„ï¼š\n",
        "{self.working_dir}/\n",
        "â”œâ”€â”€ processed/       â† è¯»å–æ­¤ç›®å½•ä¸­çš„.txtæ–‡ä»¶\n",
        "â”œâ”€â”€ jsons/          â† è¾“å‡ºçš„JSONæ–‡ä»¶å°†ä¿å­˜åˆ°è¿™é‡Œ\n",
        "â””â”€â”€ qa_generator_colab_part*.py â† å¤„ç†è„šæœ¬\n",
        "\"\"\"\n",
        "\n",
        "# é…ç½®æ—¥å¿—\n",
        "def setup_logging():\n",
        "    \"\"\"è®¾ç½®æ—¥å¿—é…ç½®\"\"\"\n",
        "    logging.basicConfig(\n",
        "        level=logging.INFO,\n",
        "        format='%(asctime)s - %(levelname)s - %(message)s',\n",
        "        force=True\n",
        "    )\n",
        "    return logging.getLogger(__name__)\n",
        "\n",
        "# å…¨å±€å˜é‡\n",
        "logger = setup_logging()\n",
        "config = ColabConfig()\n",
        "\n",
        "# åˆå§‹åŒ–ç¯å¢ƒ\n",
        "def initialize_environment():\n",
        "    \"\"\"åˆå§‹åŒ–Colabç¯å¢ƒ\"\"\"\n",
        "    print(\"=\" * 50)\n",
        "    print(\"GMPé—®ç­”å¯¹ç”Ÿæˆå™¨ - Colabç‰ˆæœ¬\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # è®¾ç½®å·¥ä½œç›®å½•\n",
        "    config.setup_directories()\n",
        "\n",
        "    # æ˜¾ç¤ºä¸Šä¼ è¯´æ˜\n",
        "    print(config.get_upload_instructions())\n",
        "\n",
        "    # æ£€æŸ¥Colab AI\n",
        "    if config.colab_available:\n",
        "        print(\"âœ“ å°†ä½¿ç”¨Google Colabå†…ç½®AI\")\n",
        "    else:\n",
        "        print(\"âš  å°†ä½¿ç”¨å¤‡ç”¨APIé…ç½®\")\n",
        "\n",
        "    print(\"\\nç¯å¢ƒåˆå§‹åŒ–å®Œæˆï¼\")\n",
        "    return True\n",
        "\n",
        "# ä½¿ç”¨è¯´æ˜\n",
        "def show_usage_instructions():\n",
        "    \"\"\"æ˜¾ç¤ºä½¿ç”¨è¯´æ˜\"\"\"\n",
        "    instructions = \"\"\"\n",
        "ä½¿ç”¨æ­¥éª¤ï¼š\n",
        "\n",
        "1. ğŸ“ ä¸Šä¼ æ–‡ä»¶ï¼šå°†.txtæ–‡ä»¶ä¸Šä¼ åˆ° /content/gmps/ ç›®å½•\n",
        "2. ğŸš€ è¿è¡Œå¤„ç†ï¼šæ‰§è¡Œç¬¬4éƒ¨åˆ†çš„ä»£ç \n",
        "3. ğŸ“Š æŸ¥çœ‹ç»“æœï¼šåœ¨ /content/jsons/ ç›®å½•ä¸­æŸ¥çœ‹ç”Ÿæˆçš„JSONæ–‡ä»¶\n",
        "\n",
        "å‘½ä»¤ç¤ºä¾‹ï¼š\n",
        "# å¤„ç†å•ä¸ªæ–‡ä»¶\n",
        "process_single_file(\"æ–‡ä»¶å.txt\")\n",
        "\n",
        "# å¤„ç†æ‰€æœ‰æ–‡ä»¶\n",
        "process_all_files()\n",
        "\n",
        "# æŸ¥çœ‹å¤„ç†è¿›åº¦\n",
        "show_progress()\n",
        "\"\"\"\n",
        "    print(instructions)\n",
        "\n",
        "# è¿è¡Œåˆå§‹åŒ–\n",
        "if __name__ == \"__main__\":\n",
        "    initialize_environment()\n",
        "    show_usage_instructions()"
      ],
      "metadata": {
        "id": "YQ0z28Xc40E_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c543731-1fc6-43c7-bed0-58338f4e7bf6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================================\n",
            "GMPé—®ç­”å¯¹ç”Ÿæˆå™¨ - Colabç‰ˆæœ¬\n",
            "==================================================\n",
            "âœ“ æ‰¾åˆ°processedç›®å½•: /content/processed\n",
            "âœ“ æ‰¾åˆ°jsonsç›®å½•: /content/jsons\n",
            "âœ“ å½“å‰å·¥ä½œç›®å½•: /content\n",
            "\n",
            "æ–‡ä»¶å¤„ç†è¯´æ˜ï¼š\n",
            "\n",
            "1. è¾“å…¥æ–‡ä»¶: ä» processed/ ç›®å½•è¯»å–.txtæ–‡ä»¶\n",
            "2. è¾“å‡ºæ–‡ä»¶: ç”Ÿæˆçš„JSONæ–‡ä»¶ä¿å­˜åˆ° jsons/ ç›®å½•\n",
            "\n",
            "å½“å‰ç›®å½•ç»“æ„ï¼š\n",
            "/content/\n",
            "â”œâ”€â”€ processed/       â† è¯»å–æ­¤ç›®å½•ä¸­çš„.txtæ–‡ä»¶\n",
            "â”œâ”€â”€ jsons/          â† è¾“å‡ºçš„JSONæ–‡ä»¶å°†ä¿å­˜åˆ°è¿™é‡Œ\n",
            "â””â”€â”€ qa_generator_colab_part*.py â† å¤„ç†è„šæœ¬\n",
            "\n",
            "âœ“ å°†ä½¿ç”¨Google Colabå†…ç½®AI\n",
            "\n",
            "ç¯å¢ƒåˆå§‹åŒ–å®Œæˆï¼\n",
            "\n",
            "ä½¿ç”¨æ­¥éª¤ï¼š\n",
            "\n",
            "1. ğŸ“ ä¸Šä¼ æ–‡ä»¶ï¼šå°†.txtæ–‡ä»¶ä¸Šä¼ åˆ° /content/gmps/ ç›®å½•\n",
            "2. ğŸš€ è¿è¡Œå¤„ç†ï¼šæ‰§è¡Œç¬¬4éƒ¨åˆ†çš„ä»£ç \n",
            "3. ğŸ“Š æŸ¥çœ‹ç»“æœï¼šåœ¨ /content/jsons/ ç›®å½•ä¸­æŸ¥çœ‹ç”Ÿæˆçš„JSONæ–‡ä»¶\n",
            "\n",
            "å‘½ä»¤ç¤ºä¾‹ï¼š\n",
            "# å¤„ç†å•ä¸ªæ–‡ä»¶\n",
            "process_single_file(\"æ–‡ä»¶å.txt\")\n",
            "\n",
            "# å¤„ç†æ‰€æœ‰æ–‡ä»¶\n",
            "process_all_files()\n",
            "\n",
            "# æŸ¥çœ‹å¤„ç†è¿›åº¦\n",
            "show_progress()\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import time\n",
        "import logging\n",
        "import re\n",
        "from pathlib import Path\n",
        "from typing import List, Dict, Optional\n",
        "from dataclasses import dataclass\n",
        "import threading\n",
        "from datetime import datetime\n",
        "import requests\n",
        "import asyncio\n",
        "from concurrent.futures import Future\n",
        "\n",
        "# è®¾ç½®é»˜è®¤loggerï¼ˆå¦‚æœpart1æ²¡æœ‰è®¾ç½®ï¼‰\n",
        "logger = logging.getLogger(__name__)\n",
        "if not logger.handlers:\n",
        "    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "@dataclass\n",
        "class ProcessingStats:\n",
        "    \"\"\"å¤„ç†ç»Ÿè®¡ä¿¡æ¯\"\"\"\n",
        "    total_files: int = 0\n",
        "    processed_files: int = 0\n",
        "    total_qa_pairs: int = 0\n",
        "    total_chunks: int = 0\n",
        "    errors: int = 0\n",
        "    start_time: float = 0\n",
        "\n",
        "    def get_progress(self) -> float:\n",
        "        \"\"\"è·å–å¤„ç†è¿›åº¦ç™¾åˆ†æ¯”\"\"\"\n",
        "        if self.total_files == 0:\n",
        "            return 0.0\n",
        "        return (self.processed_files / self.total_files) * 100\n",
        "\n",
        "class ProgressDisplay:\n",
        "    \"\"\"è¿›åº¦æ˜¾ç¤ºç±» - å•è¡Œåˆ·æ–°æ¨¡å¼\"\"\"\n",
        "    def __init__(self):\n",
        "        self.stats = ProcessingStats()\n",
        "        self.lock = threading.Lock()\n",
        "        self.current_filename = \"\"\n",
        "        self.last_update_time = 0\n",
        "\n",
        "    def start(self, total_files: int):\n",
        "        \"\"\"å¼€å§‹å¤„ç†\"\"\"\n",
        "        with self.lock:\n",
        "            self.stats.total_files = total_files\n",
        "            self.stats.start_time = time.time()\n",
        "            self.last_update_time = time.time()\n",
        "            logger.info(f\"å¼€å§‹å¤„ç† {total_files} ä¸ªæ–‡ä»¶...\")\n",
        "            print(f\"ğŸš€ å¼€å§‹å¤„ç† {total_files} ä¸ªæ–‡ä»¶...\", end=\"\")\n",
        "\n",
        "    def update_file(self, filename: str, qa_count: int = 0):\n",
        "        \"\"\"æ›´æ–°æ–‡ä»¶å¤„ç†çŠ¶æ€ï¼ˆå•è¡Œæ˜¾ç¤ºï¼‰\"\"\"\n",
        "        with self.lock:\n",
        "            self.stats.processed_files += 1\n",
        "            self.stats.total_qa_pairs += qa_count\n",
        "            progress = self.stats.get_progress()\n",
        "            self.current_filename = filename\n",
        "\n",
        "            # è®¡ç®—å¤„ç†é€Ÿåº¦\n",
        "            total_time = time.time() - self.stats.start_time\n",
        "            if total_time > 0:\n",
        "                speed = self.stats.processed_files / total_time\n",
        "                eta = (self.stats.total_files - self.stats.processed_files) / speed if speed > 0 else 0\n",
        "            else:\n",
        "                speed = 0\n",
        "                eta = 0\n",
        "\n",
        "            # æ„å»ºè¿›åº¦æ¡\n",
        "            bar_length = 20\n",
        "            filled_length = int(bar_length * self.stats.processed_files // self.stats.total_files)\n",
        "            bar = 'â–ˆ' * filled_length + '-' * (bar_length - filled_length)\n",
        "\n",
        "            # å•è¡Œåˆ·æ–°è¿›åº¦\n",
        "            progress_line = (\n",
        "                f\"\\rğŸ“Š è¿›åº¦: [{bar}] {progress:.1f}% \"\n",
        "                f\"({self.stats.processed_files}/{self.stats.total_files}) \"\n",
        "                f\"Q&A: {self.stats.total_qa_pairs} \"\n",
        "                f\"é€Ÿåº¦: {speed:.2f}æ–‡ä»¶/s \"\n",
        "                f\"ETA: {eta:.0f}s \"\n",
        "                f\"å½“å‰: {filename}\"\n",
        "            )\n",
        "\n",
        "            print(progress_line, end=\"\", flush=True)\n",
        "            self.last_update_time = time.time()\n",
        "\n",
        "    def update_chunk_progress(self, current_chunk: int, total_chunks: int, filename: str):\n",
        "        \"\"\"æ›´æ–°æ–‡æœ¬å—å¤„ç†è¿›åº¦\"\"\"\n",
        "        if total_chunks <= 0:\n",
        "            return\n",
        "\n",
        "        # é™åˆ¶æ›´æ–°é¢‘ç‡ï¼Œé¿å…åˆ·å±\n",
        "        current_time = time.time()\n",
        "        if current_time - self.last_update_time < 0.5:  # 0.5ç§’æ›´æ–°ä¸€æ¬¡\n",
        "            return\n",
        "\n",
        "        progress = (current_chunk / total_chunks) * 100\n",
        "        file_progress = ((self.stats.processed_files) / self.stats.total_files) * 100\n",
        "\n",
        "        # æ„å»ºè¿›åº¦æ¡\n",
        "        bar_length = 15\n",
        "        filled_length = int(bar_length * current_chunk // total_chunks)\n",
        "        chunk_bar = 'â–“' * filled_length + 'â–‘' * (bar_length - filled_length)\n",
        "\n",
        "        # å•è¡Œåˆ·æ–°æ–‡æœ¬å—è¿›åº¦\n",
        "        progress_line = (\n",
        "            f\"\\rğŸ“ {filename} [{chunk_bar}] {progress:.0f}% \"\n",
        "            f\"({current_chunk}/{total_chunks}å—) \"\n",
        "            f\"æ€»è¿›åº¦: {file_progress:.1f}% \"\n",
        "            f\"Q&A: {self.stats.total_qa_pairs}\"\n",
        "        )\n",
        "\n",
        "        print(progress_line, end=\"\", flush=True)\n",
        "        self.last_update_time = current_time\n",
        "\n",
        "    def add_error(self):\n",
        "        \"\"\"å¢åŠ é”™è¯¯è®¡æ•°\"\"\"\n",
        "        with self.lock:\n",
        "            self.stats.errors += 1\n",
        "\n",
        "    def update_chunks(self, chunk_count: int):\n",
        "        \"\"\"æ›´æ–°å¤„ç†çš„æ–‡æœ¬å—æ•°é‡\"\"\"\n",
        "        with self.lock:\n",
        "            self.stats.total_chunks += chunk_count\n",
        "\n",
        "    def finish_file(self, filename: str, qa_count: int = 0):\n",
        "        \"\"\"å®Œæˆæ–‡ä»¶å¤„ç†\"\"\"\n",
        "        with self.lock:\n",
        "            self.stats.total_qa_pairs += qa_count\n",
        "            # åˆ·æ–°æœ€ç»ˆçŠ¶æ€\n",
        "            self.update_file(f\"âœ… {filename}\", qa_count)\n",
        "\n",
        "    def show_summary(self):\n",
        "        \"\"\"æ˜¾ç¤ºå¤„ç†æ€»ç»“\"\"\"\n",
        "        total_time = time.time() - self.stats.start_time\n",
        "        avg_speed = self.stats.processed_files / total_time if total_time > 0 else 0\n",
        "\n",
        "        print(\"\\n\" + \"=\" * 60)\n",
        "        print(\"ğŸ‰ å¤„ç†å®Œæˆæ€»ç»“:\")\n",
        "        print(\"-\" * 60)\n",
        "        print(f\"ğŸ“ æ€»æ–‡ä»¶æ•°: {self.stats.total_files}\")\n",
        "        print(f\"âœ… æˆåŠŸå¤„ç†: {self.stats.processed_files}\")\n",
        "        print(f\"âŒ å¤„ç†å¤±è´¥: {self.stats.errors}\")\n",
        "        print(f\"ğŸ’¬ æ€»Q&Aå¯¹æ•°: {self.stats.total_qa_pairs:,}\")\n",
        "        print(f\"ğŸ“„ æ€»æ–‡æœ¬å—æ•°: {self.stats.total_chunks}\")\n",
        "        print(f\"â±ï¸  æ€»è€—æ—¶: {total_time:.2f}ç§’\")\n",
        "        print(f\"âš¡ å¹³å‡é€Ÿåº¦: {avg_speed:.2f}æ–‡ä»¶/ç§’\")\n",
        "\n",
        "        if self.stats.processed_files > 0:\n",
        "            avg_qa_per_file = self.stats.total_qa_pairs / self.stats.processed_files\n",
        "            avg_qa_per_chunk = self.stats.total_qa_pairs / self.stats.total_chunks if self.stats.total_chunks > 0 else 0\n",
        "            print(f\"ğŸ“Š å¹³å‡Q&A/æ–‡ä»¶: {avg_qa_per_file:.1f}\")\n",
        "            print(f\"ğŸ“Š å¹³å‡Q&A/æ–‡æœ¬å—: {avg_qa_per_chunk:.1f}\")\n",
        "\n",
        "        print(\"-\" * 60)\n",
        "\n",
        "        # æ˜¾ç¤ºè¾“å‡ºç›®å½•\n",
        "        try:\n",
        "            from part1 import config\n",
        "            print(f\"ğŸ“‚ ç»“æœæ–‡ä»¶ä¿å­˜åœ¨: {config.jsons_dir}/\")\n",
        "        except:\n",
        "            print(f\"ğŸ“‚ ç»“æœæ–‡ä»¶ä¿å­˜åœ¨: ./jsons/\")\n",
        "\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "# Helper to run async functions in a new thread with a new event loop\n",
        "def _run_async_in_new_thread(coro, result_future):\n",
        "    loop = asyncio.new_event_loop()\n",
        "    asyncio.set_event_loop(loop)\n",
        "    try:\n",
        "        result = loop.run_until_complete(coro)\n",
        "        result_future.set_result(result)\n",
        "    except Exception as e:\n",
        "        result_future.set_exception(e)\n",
        "    finally:\n",
        "        loop.close()\n",
        "\n",
        "class ColabAIClient:\n",
        "    \"\"\"Google Colab AIå®¢æˆ·ç«¯\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.colab_available = False\n",
        "        self.api_client = None\n",
        "        try:\n",
        "            from google.colab import ai\n",
        "            self.colab_available = True\n",
        "            logger.info(\"âœ“ Google Colab AIå®¢æˆ·ç«¯å·²åˆå§‹åŒ–\")\n",
        "        except ImportError:\n",
        "            logger.warning(\"âš  æœªæ£€æµ‹åˆ°Colabç¯å¢ƒï¼Œå°†ä½¿ç”¨å¤‡ç”¨å®¢æˆ·ç«¯\")\n",
        "            self._init_fallback_client()\n",
        "\n",
        "    def _init_fallback_client(self):\n",
        "        \"\"\"åˆå§‹åŒ–å¤‡ç”¨APIå®¢æˆ·ç«¯\"\"\"\n",
        "        try:\n",
        "            # å¦‚æœæä¾›äº†å¤‡ç”¨APIé…ç½®\n",
        "            API_BASE_URL = os.getenv(\"API_BASE_URL\", \"http://localhost:3033\")\n",
        "            API_TOKEN = os.getenv(\"API_TOKEN\", \"\")\n",
        "            if API_TOKEN:\n",
        "                self.api_client = type('APIClient', (), {\n",
        "                    'base_url': API_BASE_URL,\n",
        "                    'api_token': API_TOKEN\n",
        "                })()\n",
        "                logger.info(\"âœ“ å¤‡ç”¨APIå®¢æˆ·ç«¯å·²åˆå§‹åŒ–\")\n",
        "            else:\n",
        "                logger.warning(\"âš  æœªé…ç½®å¤‡ç”¨APIï¼Œå°†ä½¿ç”¨æ¨¡æ‹Ÿæ¨¡å¼\")\n",
        "                self.api_client = None\n",
        "        except Exception as e:\n",
        "            logger.error(f\"å¤‡ç”¨APIåˆå§‹åŒ–å¤±è´¥: {e}\")\n",
        "\n",
        "    async def generate_qa_pairs_async(self, prompt: str) -> Dict:\n",
        "        \"\"\"å¼‚æ­¥ç”Ÿæˆé—®ç­”å¯¹\"\"\"\n",
        "        if self.colab_available:\n",
        "            try:\n",
        "                from google.colab import ai\n",
        "                # Removed 'await' as ai.generate_text is a synchronous function\n",
        "                response_content = ai.generate_text(prompt)\n",
        "                return {\n",
        "                    \"content\": response_content, # Changed from response.text to response_content\n",
        "                    \"model\": \"google-colab-ai\",\n",
        "                    \"success\": True\n",
        "                }\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Colab AIè°ƒç”¨å¤±è´¥: {e}\")\n",
        "                return await self._fallback_generation(prompt)\n",
        "        else:\n",
        "            return await self._fallback_generation(prompt)\n",
        "\n",
        "    async def _fallback_generation(self, prompt: str) -> Dict:\n",
        "        \"\"\"å¤‡ç”¨ç”Ÿæˆæ–¹æ³•\"\"\"\n",
        "        if self.api_client:\n",
        "            try:\n",
        "                # æ¨¡æ‹ŸAPIè°ƒç”¨\n",
        "                return {\n",
        "                    \"content\": self._simulate_ai_response(prompt),\n",
        "                    \"model\": \"fallback-simulation\",\n",
        "                    \"success\": True\n",
        "                }\n",
        "            except Exception as e:\n",
        "                logger.error(f\"å¤‡ç”¨APIè°ƒç”¨å¤±è´¥: {e}\")\n",
        "                return {\"content\": \"\", \"model\": \"none\", \"success\": False}\n",
        "        else:\n",
        "            # æ¨¡æ‹Ÿæ¨¡å¼\n",
        "            logger.warning(\"ä½¿ç”¨æ¨¡æ‹Ÿæ¨¡å¼ç”Ÿæˆé—®ç­”å¯¹\")\n",
        "            return {\n",
        "                \"content\": self._simulate_ai_response(prompt),\n",
        "                \"model\": \"simulation\",\n",
        "                \"success\": True\n",
        "            }\n",
        "\n",
        "    def _simulate_ai_response(self, prompt: str) -> str:\n",
        "        \"\"\"æ¨¡æ‹ŸAIå“åº”ï¼ˆä»…ç”¨äºæ¼”ç¤ºï¼‰\"\"\"\n",
        "        return '''Q: ä»€ä¹ˆæ˜¯GMPï¼Ÿ\n",
        "A: GMPï¼ˆGood Manufacturing Practiceï¼‰æ˜¯è¯å“ç”Ÿäº§è´¨é‡ç®¡ç†è§„èŒƒçš„ç®€ç§°ï¼Œæ˜¯ä¸€å¥—ç”¨äºç¡®ä¿è¯å“è´¨é‡çš„æ³•è§„å’Œæ ‡å‡†ã€‚\n",
        "\n",
        "Q: GMPçš„åŸºæœ¬è¦æ±‚æ˜¯ä»€ä¹ˆï¼Ÿ\n",
        "A: GMPçš„åŸºæœ¬è¦æ±‚åŒ…æ‹¬ï¼šäººå‘˜èµ„è´¨ã€å‚æˆ¿è®¾æ–½ã€è®¾å¤‡ç»´æŠ¤ã€ç‰©æ–™ç®¡ç†ã€ç”Ÿäº§è¿‡ç¨‹æ§åˆ¶ã€è´¨é‡æ£€éªŒã€æ–‡ä»¶è®°å½•ã€è´¨é‡ä½“ç³»ç­‰æ–¹é¢ã€‚\n",
        "\n",
        "Q: è¯å“ç”Ÿäº§ä¼ä¸šéœ€è¦éµå®ˆå“ªäº›GMPæ³•è§„ï¼Ÿ\n",
        "A: è¯å“ç”Ÿäº§ä¼ä¸šéœ€è¦éµå®ˆå›½å®¶è¯å“ç›‘ç£ç®¡ç†å±€å‘å¸ƒçš„ã€Šè¯å“ç”Ÿäº§è´¨é‡ç®¡ç†è§„èŒƒã€‹ä»¥åŠç›¸å…³çš„æŠ€æœ¯æŒ‡å—å’Œè¦æ±‚ã€‚'''\n",
        "\n",
        "    def generate_qa_pairs(self, prompt: str) -> Dict:\n",
        "        \"\"\"åŒæ­¥ç”Ÿæˆé—®ç­”å¯¹ï¼ˆå…¼å®¹æ€§æ–¹æ³•ï¼‰ï¼Œé€šè¿‡æ–°çº¿ç¨‹é¿å…äº‹ä»¶å¾ªç¯å†²çª\"\"\"\n",
        "        result_future = Future()\n",
        "        thread = threading.Thread(target=_run_async_in_new_thread, args=(self.generate_qa_pairs_async(prompt), result_future))\n",
        "        thread.start()\n",
        "        thread.join() # Ensure the thread completes before returning\n",
        "        return result_future.result() # Blocks until the async call is complete\n",
        "\n",
        "class QAGeneratorColab:\n",
        "    \"\"\"Colabç‰ˆæœ¬çš„é—®ç­”å¯¹ç”Ÿæˆå™¨\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.ai_client = ColabAIClient()\n",
        "        self.progress = ProgressDisplay()\n",
        "\n",
        "        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨\n",
        "        # é…ç½®å°†åœ¨åˆå§‹åŒ–æ—¶è®¾ç½®\n",
        "        self.config = None\n",
        "        os.makedirs(\"/content/jsons\", exist_ok=True)\n",
        "\n",
        "    def generate_qa_pairs(self, text: str) -> List[Dict]:\n",
        "        \"\"\"ç”Ÿæˆé—®ç­”å¯¹\"\"\"\n",
        "        if len(text.strip()) < 50:\n",
        "            logger.warning(\"æ–‡æœ¬è¿‡çŸ­ï¼Œè·³è¿‡å¤„ç†\")\n",
        "            return []\n",
        "\n",
        "        # æ„å»ºæç¤ºè¯\n",
        "        prompt = f\"\"\"ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–‡æœ¬åˆ†æåŠ©æ‰‹ï¼Œæ“…é•¿ä»æŠ€æœ¯æ–‡æ¡£ä¸­æå–å…³é”®ä¿¡æ¯å¹¶ç»„ç»‡æˆæ¸…æ™°çš„é—®ç­”å¯¹ã€‚\n",
        "\n",
        "ã€ä»»åŠ¡ã€‘\n",
        "è¯·ä»”ç»†é˜…è¯»ä»¥ä¸‹æ–‡æœ¬ï¼Œå¹¶ä»ä¸­ç”Ÿæˆä¸€ç³»åˆ—æœ‰æ„ä¹‰çš„é—®ç­”å¯¹ã€‚é—®ç­”å¯¹åº”è¦†ç›–æ–‡æœ¬ä¸­çš„æ ¸å¿ƒæ¦‚å¿µã€å…³é”®æµç¨‹ã€ç›‘ç®¡è¦æ±‚å’Œé‡è¦ç»“è®ºã€‚\n",
        "\n",
        "ã€ç”Ÿæˆè¦æ±‚ã€‘\n",
        "1. **é—®é¢˜**ï¼šåº”åŸºäºæ–‡æœ¬å†…å®¹æå‡ºï¼Œé—®é¢˜æ˜ç¡®ã€å…·ä½“ï¼Œèƒ½æŠ“ä½ä¸€ä¸ªç‹¬ç«‹çš„çŸ¥è¯†ç‚¹ã€‚\n",
        "2. **ç­”æ¡ˆ**ï¼šå¿…é¡»ç›´æ¥ä»æ–‡æœ¬ä¸­æç‚¼å’Œæ€»ç»“ï¼Œç¡®ä¿ä¿¡æ¯å‡†ç¡®ã€å®Œæ•´ã€‚\n",
        "3. **æ•°é‡ä¸è´¨é‡**ï¼šç”Ÿæˆ2-10ä¸ªé«˜è´¨é‡çš„é—®ç­”å¯¹ï¼Œç¡®ä¿è¦†ç›–æ–‡æœ¬çš„ä¸åŒéƒ¨åˆ†ã€‚\n",
        "4. **æ ¼å¼**ï¼šè¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹æ ¼å¼è¾“å‡ºï¼š\n",
        "\n",
        "Q: [è¿™é‡Œå†™é—®é¢˜]\n",
        "A: [è¿™é‡Œå†™ç­”æ¡ˆ]\n",
        "\n",
        "ã€æ–‡æœ¬ã€‘\n",
        "{text}\n",
        "\n",
        "ç°åœ¨ï¼Œè¯·å¼€å§‹ç”Ÿæˆé—®ç­”å¯¹ï¼š\"\"\"\n",
        "\n",
        "        # Suppress logging during AI call to avoid breaking progress bar\n",
        "        original_level = logging.getLogger().level\n",
        "        logging.getLogger().setLevel(logging.ERROR) # Temporarily set to ERROR to suppress INFO/WARNING\n",
        "\n",
        "        try:\n",
        "            # è°ƒç”¨AI\n",
        "            result = self.ai_client.generate_qa_pairs(prompt)\n",
        "\n",
        "            if not result[\"success\"]:\n",
        "                logger.error(f\"AIè°ƒç”¨å¤±è´¥: {result}\")\n",
        "                return []\n",
        "\n",
        "            # æå–å›å¤å†…å®¹\n",
        "            response_text = result[\"content\"]\n",
        "            # logger.info(f\"ä½¿ç”¨æ¨¡å‹: {result['model']}\") # Suppress this log during progress update\n",
        "\n",
        "            # æå–é—®ç­”å¯¹\n",
        "            qa_pairs = self._extract_qa_pairs_from_response(response_text)\n",
        "\n",
        "            if qa_pairs:\n",
        "                # logger.info(f\"æˆåŠŸç”Ÿæˆ {len(qa_pairs)} ä¸ªé—®ç­”å¯¹\") # Suppress this log during progress update\n",
        "                pass\n",
        "            else:\n",
        "                # logger.warning(\"æœªèƒ½ä»å“åº”ä¸­æå–æœ‰æ•ˆçš„é—®ç­”å¯¹\") # Suppress this log during progress update\n",
        "                pass\n",
        "            return qa_pairs\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"ç”Ÿæˆé—®ç­”å¯¹æ—¶å‡ºé”™: {str(e)}\")\n",
        "            return []\n",
        "        finally:\n",
        "            logging.getLogger().setLevel(original_level) # Restore original logging level\n",
        "\n",
        "    def _extract_qa_pairs_from_response(self, response_text: str) -> List[Dict]:\n",
        "        \"\"\"ä»å“åº”ä¸­æå–é—®ç­”å¯¹\"\"\"\n",
        "        try:\n",
        "            # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼åŒ¹é… Q: ... A: ... æ ¼å¼\n",
        "            qa_pairs = []\n",
        "            qa_pattern = r'Q:\\s*(.+?)\\s*A:\\s*([^Q]+?)(?=\\s*Q:|$)'\n",
        "            matches = re.findall(qa_pattern, response_text, re.DOTALL)\n",
        "\n",
        "            for match in matches:\n",
        "                question = match[0].strip()\n",
        "                answer = match[1].strip()\n",
        "\n",
        "                if question and answer and len(answer) > 10:\n",
        "                    qa_pairs.append({\n",
        "                        \"instruction\": question,\n",
        "                        \"input\": \"\",\n",
        "                        \"output\": answer\n",
        "                    })\n",
        "                    # logger.info(f\"æå–Q&Aå¯¹: {question[:50]}...\") # Suppress this log during progress update\n",
        "\n",
        "            if qa_pairs:\n",
        "                # logger.info(f\"æˆåŠŸè§£æ {len(qa_pairs)} ä¸ªé—®ç­”å¯¹\") # Suppress this log during progress update\n",
        "                pass\n",
        "            else:\n",
        "                # logger.debug(\"æœªæ‰¾åˆ°æœ‰æ•ˆçš„Q:Aæ ¼å¼é—®ç­”å¯¹\") # Suppress this log during progress update\n",
        "                pass\n",
        "            return qa_pairs\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"æå–é—®ç­”å¯¹æ—¶å‡ºé”™: {str(e)}\")\n",
        "            return []\n",
        "\n",
        "    def save_qa_pairs(self, filename: str, qa_pairs: List[Dict]) -> bool:\n",
        "        \"\"\"ä¿å­˜é—®ç­”å¯¹åˆ°JSONæ–‡ä»¶\"\"\"\n",
        "        output_path = Path(\"/content/jsons\") / f\"{filename}.json\"\n",
        "\n",
        "        try:\n",
        "            with open(output_path, 'w', encoding='utf-8') as f:\n",
        "                json.dump(qa_pairs, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "            # logger.info(f\"æˆåŠŸä¿å­˜ {len(qa_pairs)} ä¸ªé—®ç­”å¯¹åˆ° {output_path}\") # Suppress this log during progress update\n",
        "            return True\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"ä¿å­˜é—®ç­”å¯¹æ—¶å‡ºé”™: {str(e)}\")\n",
        "            return False\n",
        "\n",
        "    def append_qa_pairs(self, filename: str, qa_pairs: List[Dict]) -> bool:\n",
        "        \"\"\"è¿½åŠ é—®ç­”å¯¹åˆ°JSONæ–‡ä»¶\"\"\"\n",
        "        output_path = Path(\"/content/jsons\") / f\"{filename}.json\"\n",
        "        temp_path = output_path.with_suffix('.tmp')\n",
        "\n",
        "        try:\n",
        "            # è¯»å–ç°æœ‰æ•°æ®\n",
        "            existing_data = []\n",
        "            if output_path.exists():\n",
        "                with open(output_path, 'r', encoding='utf-8') as f:\n",
        "                    existing_data = json.load(f)\n",
        "\n",
        "            # è¿½åŠ æ–°æ•°æ®\n",
        "            existing_data.extend(qa_pairs)\n",
        "\n",
        "            # å†™å…¥ä¸´æ—¶æ–‡ä»¶\n",
        "            with open(temp_path, 'w', encoding='utf-8') as f:\n",
        "                json.dump(existing_data, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "            # é‡å‘½åä¸´æ—¶æ–‡ä»¶\n",
        "            temp_path.replace(output_path)\n",
        "\n",
        "            # logger.info(f\"å¢é‡ä¿å­˜æˆåŠŸ: {len(qa_pairs)} ä¸ªé—®ç­”å¯¹å·²è¿½åŠ åˆ° {output_path.name}\") # Suppress this log during progress update\n",
        "            # logger.info(f\"å½“å‰æ€»é—®ç­”å¯¹æ•°: {len(existing_data)}\") # Suppress this log during progress update\n",
        "            return True\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"ä¿å­˜é—®ç­”å¯¹æ—¶å‡ºé”™: {str(e)}\")\n",
        "            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶\n",
        "            if temp_path.exists():\n",
        "                temp_path.unlink()\n",
        "            return False\n",
        "\n",
        "# å¯¼å‡ºä¸»è¦ç±»\n",
        "__all__ = ['QAGeneratorColab', 'ProcessingStats', 'ProgressDisplay', 'ColabAIClient']"
      ],
      "metadata": {
        "id": "lR91Pc147_da"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "GMPé—®ç­”å¯¹ç”Ÿæˆæ¨¡å— - Colabç‰ˆæœ¬\n",
        "ç¬¬3éƒ¨åˆ†ï¼šæ–‡æœ¬å¤„ç†å’Œåˆ†å—åŠŸèƒ½\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import re\n",
        "from pathlib import Path\n",
        "from typing import List, Dict, Optional\n",
        "import logging\n",
        "\n",
        "# è®¾ç½®é»˜è®¤loggerï¼ˆå¦‚æœpart1æ²¡æœ‰è®¾ç½®ï¼‰\n",
        "logger = logging.getLogger(__name__)\n",
        "if not logger.handlers:\n",
        "    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "class TextProcessor:\n",
        "    \"\"\"æ–‡æœ¬å¤„ç†å™¨ - ä¸“é—¨å¤„ç†ä¸­æ–‡GMPæ–‡æ¡£\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.chinese_punctuation = ['ã€‚', 'ï¼', 'ï¼Ÿ', 'ï¼›', 'ï¼š', '\"', '\"', ''', ''', 'ï¼ˆ', 'ï¼‰', 'ã€', 'ã€‘']\n",
        "        self.line_endings = ['\\n', '\\r', '\\r\\n']\n",
        "\n",
        "    def clean_text(self, text: str) -> str:\n",
        "        \"\"\"æ¸…ç†æ–‡æœ¬ï¼Œå»é™¤OCRé”™è¯¯å’Œæ ¼å¼é—®é¢˜\"\"\"\n",
        "        if not text:\n",
        "            return \"\"\n",
        "\n",
        "        # å»é™¤ç©ºç™½å­—ç¬¦ä½†ä¿ç•™ä¸­æ–‡æ ¼å¼\n",
        "        text = re.sub(r'[ \\t]+', ' ', text)  # åˆ¶è¡¨ç¬¦å’Œå¤šä¸ªç©ºæ ¼è½¬ä¸ºå•ä¸ªç©ºæ ¼\n",
        "        text = re.sub(r'\\n{3,}', '\\n\\n', text)  # å¤šä¸ªæ¢è¡Œè½¬ä¸ºä¸¤ä¸ª\n",
        "        text = re.sub(r' +\\n', '\\n', text)  # è¡Œå°¾ç©ºæ ¼å»é™¤\n",
        "        text = re.sub(r'\\n +', '\\n', text)  # è¡Œé¦–ç©ºæ ¼å»é™¤\n",
        "\n",
        "        # ä¿®å¤å¸¸è§çš„OCRé”™è¯¯\n",
        "        ocr_corrections = {\n",
        "            'â–¡': '',  # æ–¹æ¡†å­—ç¬¦\n",
        "            'â€»': '',  # å‚è€ƒæ ‡è®°\n",
        "            'â€¢': '',  # é¡¹ç›®ç¬¦å·\n",
        "            'â€¦': '...',  # çœç•¥å·ç»Ÿä¸€\n",
        "            '\"': '\"',  # å¼•å·ç»Ÿä¸€\n",
        "            '\"': '\"',\n",
        "            ''': \"'\",\n",
        "            ''': \"'\"\n",
        "        }\n",
        "\n",
        "        for wrong, correct in ocr_corrections.items():\n",
        "            text = text.replace(wrong, correct)\n",
        "\n",
        "        # å»é™¤æ§åˆ¶å­—ç¬¦ä½†ä¿ç•™ä¸­æ–‡æ ‡ç‚¹\n",
        "        text = ''.join(char for char in text if ord(char) >= 32 or char in self.chinese_punctuation)\n",
        "\n",
        "        # ä¿®å¤æ•°å­—å’Œå•ä½ä¹‹é—´çš„ç©ºæ ¼\n",
        "        text = re.sub(r'(\\d+)\\s+([ä¸ªåªç‰‡ç²’æ¯«å‡å…‹])(?![a-zA-Z])', r'\\1\\2', text)\n",
        "\n",
        "        # ä¿®å¤å¸¸è§çš„GMPæœ¯è¯­æ ¼å¼\n",
        "        text = self._fix_gmp_terms(text)\n",
        "\n",
        "        return text.strip()\n",
        "\n",
        "    def _fix_gmp_terms(self, text: str) -> str:\n",
        "        \"\"\"ä¿®å¤GMPç›¸å…³æœ¯è¯­çš„æ ¼å¼\"\"\"\n",
        "        # ä¿®å¤æ¡æ¬¾ç¼–å·æ ¼å¼\n",
        "        text = re.sub(r'ç¬¬\\s*(\\d+)\\s*æ¡', r'ç¬¬\\1æ¡', text)\n",
        "        text = re.sub(r'(\\d+)\\s*\\.\\s*(\\d+)', r'\\1.\\2', text)  # å¦‚ 1. 2 -> 1.2\n",
        "\n",
        "        # ä¿®å¤å¸¸è§çš„GMPç¼©å†™\n",
        "        gmp_terms = {\n",
        "            'G M P': 'GMP',\n",
        "            'g m p': 'GMP',\n",
        "            'H A C C P': 'HACCP',\n",
        "            'I S O': 'ISO',\n",
        "            'Q A': 'QA',\n",
        "            'Q C': 'QC'\n",
        "        }\n",
        "\n",
        "        for term, correct in gmp_terms.items():\n",
        "            text = re.sub(r'\\b' + re.escape(term) + r'\\b', correct, text, flags=re.IGNORECASE)\n",
        "\n",
        "        return text\n",
        "\n",
        "    def process_text_chunk(self, text: str, chunk_size: int = 800, overlap_size: int = 200) -> List[str]:\n",
        "        \"\"\"å°†æ–‡æœ¬åˆ†å—å¤„ç†ï¼Œä¼˜åŒ–ä¸­æ–‡æ–‡æœ¬çš„åˆ†å‰²\"\"\"\n",
        "        if not text or len(text) <= chunk_size:\n",
        "            return [text] if text.strip() else []\n",
        "\n",
        "        chunks = []\n",
        "        start = 0\n",
        "\n",
        "        while start < len(text):\n",
        "            # è®¡ç®—å—çš„ç»“æŸä½ç½®\n",
        "            end = start + chunk_size\n",
        "\n",
        "            # å¦‚æœåˆ°è¾¾æ–‡æœ¬æœ«å°¾ï¼Œç›´æ¥æ·»åŠ å‰©ä½™éƒ¨åˆ†\n",
        "            if end >= len(text):\n",
        "                chunks.append(text[start:].strip())\n",
        "                break\n",
        "\n",
        "            # å¯»æ‰¾æœ€ä½³çš„åˆ†å‰²ç‚¹\n",
        "            chunk = text[start:end]\n",
        "\n",
        "            # ä¼˜å…ˆåœ¨å¥å·ã€é—®å·ã€æ„Ÿå¹å·å¤„åˆ†å‰²\n",
        "            split_pos = self._find_best_split_point(chunk)\n",
        "\n",
        "            if split_pos != -1:\n",
        "                chunks.append(chunk[:split_pos + 1].strip())\n",
        "                start = start + split_pos + 1 - overlap_size\n",
        "            else:\n",
        "                # å¦‚æœæ‰¾ä¸åˆ°åˆé€‚çš„åˆ†å‰²ç‚¹ï¼Œå¼ºåˆ¶åˆ†å‰²\n",
        "                chunks.append(chunk.strip())\n",
        "                start = end - overlap_size\n",
        "\n",
        "        # è¿‡æ»¤ç©ºå—å’Œè¿‡çŸ­çš„å—\n",
        "        chunks = [chunk for chunk in chunks if chunk and len(chunk.strip()) > 50]\n",
        "\n",
        "        logger.info(f\"æ–‡æœ¬åˆ†å—å®Œæˆï¼Œå…± {len(chunks)} ä¸ªå—ï¼Œå¹³å‡é•¿åº¦: {sum(len(c) for c in chunks) / len(chunks):.0f} å­—ç¬¦\")\n",
        "        return chunks\n",
        "\n",
        "    def _find_best_split_point(self, chunk: str) -> int:\n",
        "        \"\"\"å¯»æ‰¾æœ€ä½³çš„åˆ†å‰²ç‚¹\"\"\"\n",
        "        # åˆ†å‰²ç‚¹ä¼˜å…ˆçº§ï¼šå¥å· > é—®å· > æ„Ÿå¹å· > åˆ†å· > å†’å· > é€—å· > æ¢è¡Œ\n",
        "\n",
        "        split_chars = ['ã€‚', 'ï¼', 'ï¼Ÿ', 'ï¼›', 'ï¼š', 'ï¼Œ', '\\n']\n",
        "\n",
        "        for char in split_chars:\n",
        "            # ä»åå‘å‰æŸ¥æ‰¾åˆ†å‰²ç‚¹\n",
        "            for i in range(len(chunk) - 1, max(0, len(chunk) - 200), -1):\n",
        "                if chunk[i] == char:\n",
        "                    return i\n",
        "\n",
        "        return -1\n",
        "\n",
        "    def extract_sentences(self, text: str) -> List[str]:\n",
        "        \"\"\"æå–å¥å­ï¼Œç”¨äºæ›´ç»†ç²’åº¦çš„å¤„ç†\"\"\"\n",
        "        if not text:\n",
        "            return []\n",
        "\n",
        "        # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼åˆ†å‰²å¥å­\n",
        "        sentences = re.split(r'[ã€‚ï¼ï¼Ÿï¼›]', text)\n",
        "\n",
        "        # æ¸…ç†å’Œè¿‡æ»¤å¥å­\n",
        "        sentences = [s.strip() for s in sentences if s.strip() and len(s.strip()) > 10]\n",
        "\n",
        "        return sentences\n",
        "\n",
        "    def detect_document_type(self, text: str) -> str:\n",
        "        \"\"\"æ£€æµ‹æ–‡æ¡£ç±»å‹\"\"\"\n",
        "        text_lower = text.lower()\n",
        "\n",
        "        if any(keyword in text_lower for keyword in ['gmp', 'good manufacturing practice', 'è¯å“ç”Ÿäº§è´¨é‡ç®¡ç†']):\n",
        "            return 'GMP'\n",
        "        elif any(keyword in text_lower for keyword in ['sop', 'æ ‡å‡†æ“ä½œè§„ç¨‹', 'æ“ä½œæŒ‡å—']):\n",
        "            return 'SOP'\n",
        "        elif any(keyword in text_lower for keyword in ['éªŒè¯', 'validation', 'ç¡®è®¤']):\n",
        "            return 'Validation'\n",
        "        elif any(keyword in text_lower for keyword in ['åŸ¹è®­', 'training', 'åŸ¹è®­è®°å½•']):\n",
        "            return 'Training'\n",
        "        elif any(keyword in text_lower for keyword in ['æ£€æŸ¥', 'inspection', 'å®¡è®¡', 'audit']):\n",
        "            return 'Inspection'\n",
        "        else:\n",
        "            return 'General'\n",
        "\n",
        "    def extract_key_info(self, text: str) -> Dict[str, List[str]]:\n",
        "        \"\"\"æå–å…³é”®ä¿¡æ¯\"\"\"\n",
        "        key_info = {\n",
        "            'requirements': [],\n",
        "            'procedures': [],\n",
        "            'responsibilities': [],\n",
        "            'definitions': []\n",
        "        }\n",
        "\n",
        "        # æå–è¦æ±‚ï¼ˆé€šå¸¸åŒ…å«\"åº”\"ã€\"å¿…é¡»\"ã€\"ä¸å¾—\"ç­‰å…³é”®è¯ï¼‰\n",
        "        requirement_patterns = [\n",
        "            r'[^ã€‚ï¼ï¼Ÿ]*?åº”[^ã€‚ï¼ï¼Ÿ]*[ã€‚ï¼ï¼Ÿ]',\n",
        "            r'[^ã€‚ï¼ï¼Ÿ]*?å¿…é¡»[^ã€‚ï¼ï¼Ÿ]*[ã€‚ï¼ï¼Ÿ]',\n",
        "            r'[^ã€‚ï¼ï¼Ÿ]*?ä¸å¾—[^ã€‚ï¼ï¼Ÿ]*[ã€‚ï¼ï¼Ÿ]',\n",
        "            r'[^ã€‚ï¼ï¼Ÿ]*?ä¸¥ç¦[^ã€‚ï¼ï¼Ÿ]*[ã€‚ï¼ï¼Ÿ]'\n",
        "        ]\n",
        "\n",
        "        for pattern in requirement_patterns:\n",
        "            matches = re.findall(pattern, text)\n",
        "            key_info['requirements'].extend(matches)\n",
        "\n",
        "        # æå–ç¨‹åºï¼ˆé€šå¸¸åŒ…å«\"ç¨‹åº\"ã€\"æ­¥éª¤\"ã€\"æ–¹æ³•\"ç­‰ï¼‰\n",
        "        procedure_patterns = [\n",
        "            r'[^ã€‚ï¼ï¼Ÿ]*?ç¨‹åº[^ã€‚ï¼ï¼Ÿ]*[ã€‚ï¼ï¼Ÿ]',\n",
        "            r'[^ã€‚ï¼ï¼Ÿ]*?æ­¥éª¤[^ã€‚ï¼ï¼Ÿ]*[ã€‚ï¼ï¼Ÿ]',\n",
        "            r'[^ã€‚ï¼ï¼Ÿ]*?æ–¹æ³•[^ã€‚ï¼ï¼Ÿ]*[ã€‚ï¼ï¼Ÿ]'\n",
        "        ]\n",
        "\n",
        "        for pattern in procedure_patterns:\n",
        "            matches = re.findall(pattern, text)\n",
        "            key_info['procedures'].extend(matches)\n",
        "\n",
        "        # æå–èŒè´£ï¼ˆé€šå¸¸åŒ…å«\"è´Ÿè´£\"ã€\"èŒè´£\"ç­‰ï¼‰\n",
        "        responsibility_patterns = [\n",
        "            r'[^ã€‚ï¼ï¼Ÿ]*?è´Ÿè´£[^ã€‚ï¼ï¼Ÿ]*[ã€‚ï¼ï¼Ÿ]',\n",
        "            r'[^ã€‚ï¼ï¼Ÿ]*?èŒè´£[^ã€‚ï¼ï¼Ÿ]*[ã€‚ï¼ï¼Ÿ]',\n",
        "            r'[^ã€‚ï¼ï¼Ÿ]*?æƒé™[^ã€‚ï¼ï¼Ÿ]*[ã€‚ï¼ï¼Ÿ]'\n",
        "        ]\n",
        "\n",
        "        for pattern in responsibility_patterns:\n",
        "            matches = re.findall(pattern, text)\n",
        "            key_info['responsibilities'].extend(matches)\n",
        "\n",
        "        # æå–å®šä¹‰ï¼ˆé€šå¸¸åŒ…å«\"å®šä¹‰\"ã€\"æ˜¯æŒ‡\"ç­‰ï¼‰\n",
        "        definition_patterns = [\n",
        "            r'[^ã€‚ï¼ï¼Ÿ]*?å®šä¹‰[^ã€‚ï¼ï¼Ÿ]*[ã€‚ï¼ï¼Ÿ]',\n",
        "            r'[^ã€‚ï¼ï¼Ÿ]*?æ˜¯æŒ‡[^ã€‚ï¼ï¼Ÿ]*[ã€‚ï¼ï¼Ÿ]',\n",
        "            r'[^ã€‚ï¼ï¼Ÿ]*?ä¸º[^ã€‚ï¼ï¼Ÿ]*[ã€‚ï¼ï¼Ÿ]'\n",
        "        ]\n",
        "\n",
        "        for pattern in definition_patterns:\n",
        "            matches = re.findall(pattern, text)\n",
        "            key_info['definitions'].extend(matches)\n",
        "\n",
        "        # å»é‡å¹¶æ¸…ç†\n",
        "        for key in key_info:\n",
        "            key_info[key] = list(set([item.strip() for item in key_info[key] if item.strip()]))\n",
        "\n",
        "        return key_info\n",
        "\n",
        "    def validate_text_quality(self, text: str) -> Dict[str, any]:\n",
        "        \"\"\"éªŒè¯æ–‡æœ¬è´¨é‡\"\"\"\n",
        "        stats = {\n",
        "            'total_chars': len(text),\n",
        "            'chinese_chars': len(re.findall(r'[\\u4e00-\\u9fff]', text)),\n",
        "            'english_chars': len(re.findall(r'[a-zA-Z]', text)),\n",
        "            'numbers': len(re.findall(r'\\d', text)),\n",
        "            'punctuation': len(re.findall(r'[ï¼Œã€‚ï¼ï¼Ÿï¼›ï¼š\"\"''ï¼ˆï¼‰ã€ã€‘]', text)),\n",
        "            'sentences': len(re.findall(r'[ã€‚ï¼ï¼Ÿ]', text)),\n",
        "            'quality_score': 0\n",
        "        }\n",
        "\n",
        "        # è®¡ç®—è´¨é‡è¯„åˆ†\n",
        "        if stats['total_chars'] > 0:\n",
        "            chinese_ratio = stats['chinese_chars'] / stats['total_chars']\n",
        "            sentence_ratio = stats['sentences'] / max(stats['total_chars'] / 50, 1)  # å‡è®¾æ¯50å­—ç¬¦ä¸€ä¸ªå¥å­\n",
        "\n",
        "            # è´¨é‡è¯„åˆ†ï¼ˆ0-100ï¼‰\n",
        "            quality_score = 0\n",
        "            if chinese_ratio > 0.7:  # ä¸­æ–‡å æ¯”è¾ƒé«˜\n",
        "                quality_score += 40\n",
        "            if sentence_ratio > 0.5:  # å¥å­æ•°é‡åˆç†\n",
        "                quality_score += 30\n",
        "            if stats['punctuation'] > 0:  # æœ‰æ ‡ç‚¹ç¬¦å·\n",
        "                quality_score += 20\n",
        "            if stats['total_chars'] > 100:  # æ–‡æœ¬é•¿åº¦è¶³å¤Ÿ\n",
        "                quality_score += 10\n",
        "\n",
        "            stats['quality_score'] = min(quality_score, 100)\n",
        "\n",
        "        return stats\n",
        "\n",
        "# æ–‡ä»¶å¤„ç†å·¥å…·ç±»\n",
        "class FileManager:\n",
        "    \"\"\"æ–‡ä»¶ç®¡ç†å™¨\"\"\"\n",
        "\n",
        "    def __init__(self, config=None):\n",
        "        self.config = config\n",
        "        # ä½¿ç”¨å½“å‰ç›®å½•ç»“æ„\n",
        "        self.jsons_dir = \"./jsons\"\n",
        "        self.processed_dir = \"./processed\"\n",
        "        self.text_processor = TextProcessor()\n",
        "\n",
        "    def load_text_file(self, file_path: Path) -> str:\n",
        "        \"\"\"åŠ è½½æ–‡æœ¬æ–‡ä»¶\"\"\"\n",
        "        try:\n",
        "            with open(file_path, 'r', encoding='utf-8') as f:\n",
        "                text = f.read()\n",
        "\n",
        "            # æ¸…ç†æ–‡æœ¬\n",
        "            cleaned_text = self.text_processor.clean_text(text)\n",
        "\n",
        "            # éªŒè¯æ–‡æœ¬è´¨é‡\n",
        "            quality_stats = self.text_processor.validate_text_quality(cleaned_text)\n",
        "            logger.info(f\"æ–‡æœ¬è´¨é‡è¯„åˆ†: {quality_stats['quality_score']}/100\")\n",
        "\n",
        "            if quality_stats['quality_score'] < 30:\n",
        "                logger.warning(\"æ–‡æœ¬è´¨é‡è¾ƒä½ï¼Œå¯èƒ½å½±å“é—®ç­”å¯¹ç”Ÿæˆè´¨é‡\")\n",
        "\n",
        "            return cleaned_text\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"åŠ è½½æ–‡ä»¶å¤±è´¥ {file_path}: {str(e)}\")\n",
        "            return \"\"\n",
        "\n",
        "    def save_processed_text(self, original_path: Path, text: str) -> Path:\n",
        "        \"\"\"ä¿å­˜å¤„ç†åçš„æ–‡æœ¬\"\"\"\n",
        "        processed_path = Path(self.processed_dir) / f\"{original_path.stem}_cleaned.txt\"\n",
        "\n",
        "        try:\n",
        "            with open(processed_path, 'w', encoding='utf-8') as f:\n",
        "                f.write(text)\n",
        "\n",
        "            logger.info(f\"å¤„ç†åçš„æ–‡æœ¬å·²ä¿å­˜: {processed_path}\")\n",
        "            return processed_path\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"ä¿å­˜å¤„ç†æ–‡æœ¬å¤±è´¥: {str(e)}\")\n",
        "            return None\n",
        "\n",
        "    def get_file_list(self, directory: str = None) -> List[Path]:\n",
        "        \"\"\"è·å–æ–‡ä»¶åˆ—è¡¨\"\"\"\n",
        "        if directory is None:\n",
        "            directory = self.processed_dir  # é»˜è®¤ä»processedç›®å½•è¯»å–\n",
        "\n",
        "        dir_path = Path(directory)\n",
        "        if not dir_path.exists():\n",
        "            logger.error(f\"ç›®å½•ä¸å­˜åœ¨: {directory}\")\n",
        "            return []\n",
        "\n",
        "        files = list(dir_path.glob(\"*.txt\"))\n",
        "        logger.info(f\"åœ¨ {directory} ä¸­æ‰¾åˆ° {len(files)} ä¸ªæ–‡æœ¬æ–‡ä»¶\")\n",
        "        return files\n",
        "\n",
        "    def check_existing_output(self, filename: str) -> bool:\n",
        "        \"\"\"æ£€æŸ¥æ˜¯å¦å·²æœ‰è¾“å‡ºæ–‡ä»¶\"\"\"\n",
        "        output_path = Path(self.jsons_dir) / f\"{filename}.json\"\n",
        "        return output_path.exists()\n",
        "\n",
        "# å¯¼å‡ºç±»\n",
        "__all__ = ['TextProcessor', 'FileManager']"
      ],
      "metadata": {
        "id": "azYpQew_8T1q"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "GMPé—®ç­”å¯¹ç”Ÿæˆæ¨¡å— - Colabç‰ˆæœ¬\n",
        "ç¬¬4éƒ¨åˆ†ï¼šä¸»è¦æ‰§è¡Œé€»è¾‘å’Œç¤ºä¾‹ç”¨æ³•\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import time\n",
        "import json\n",
        "from pathlib import Path\n",
        "from typing import List, Dict\n",
        "import logging\n",
        "\n",
        "# è®¾ç½®é»˜è®¤loggerï¼ˆå¦‚æœpart1æ²¡æœ‰è®¾ç½®ï¼‰\n",
        "logger = logging.getLogger(__name__)\n",
        "if not logger.handlers:\n",
        "    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "class GMPProcessor:\n",
        "    \"\"\"GMPæ–‡æ¡£å¤„ç†å™¨ - ä¸»è¦æ‰§è¡Œé€»è¾‘\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        # from part2 import QAGeneratorColab\n",
        "        # from part3 import FileManager, TextProcessor\n",
        "\n",
        "        # ä¸ä¾èµ–configï¼Œä½¿ç”¨å½“å‰ç›®å½•ç»“æ„\n",
        "        self.processed_dir = \"./processed\"\n",
        "        self.jsons_dir = \"./jsons\"\n",
        "\n",
        "        # ç”±äºç±»å·²ç»åœ¨execæ—¶å¯¼å…¥åˆ°å…¨å±€ï¼Œç›´æ¥ä½¿ç”¨\n",
        "        global QAGeneratorColab, FileManager, TextProcessor\n",
        "        self.qa_generator = QAGeneratorColab()\n",
        "        self.file_manager = FileManager()  # ä¸ä¼ é€’config\n",
        "        self.text_processor = TextProcessor()\n",
        "        self.progress = self.qa_generator.progress\n",
        "\n",
        "    def process_single_file(self, file_path: Path, chunk_size: int = 800, overlap_size: int = 200) -> bool:\n",
        "        \"\"\"å¤„ç†å•ä¸ªæ–‡ä»¶\"\"\"\n",
        "        try:\n",
        "            logger.info(f\"å¼€å§‹å¤„ç†æ–‡ä»¶: {file_path.name}\")\n",
        "\n",
        "            # åŠ è½½å’Œæ¸…ç†æ–‡æœ¬\n",
        "            raw_text = self.file_manager.load_text_file(file_path)\n",
        "            if not raw_text:\n",
        "                logger.error(f\"æ— æ³•åŠ è½½æ–‡ä»¶: {file_path}\")\n",
        "                return False\n",
        "\n",
        "            # æ£€æµ‹æ–‡æ¡£ç±»å‹\n",
        "            doc_type = self.text_processor.detect_document_type(raw_text)\n",
        "            logger.info(f\"æ–‡æ¡£ç±»å‹: {doc_type}\")\n",
        "\n",
        "            # ä¿å­˜æ¸…ç†åçš„æ–‡æœ¬\n",
        "            self.file_manager.save_processed_text(file_path, raw_text)\n",
        "\n",
        "            # æ–‡æœ¬åˆ†å—\n",
        "            chunks = self.text_processor.process_text_chunk(raw_text, chunk_size, overlap_size)\n",
        "            if not chunks:\n",
        "                logger.warning(\"æ–‡æœ¬åˆ†å—åä¸ºç©º\")\n",
        "                return False\n",
        "\n",
        "            self.progress.update_chunks(len(chunks))\n",
        "\n",
        "            # å¤„ç†æ¯ä¸ªæ–‡æœ¬å—\n",
        "            total_qa_pairs = []\n",
        "            filename = file_path.stem\n",
        "\n",
        "            for i, chunk in enumerate(chunks):\n",
        "                # æ˜¾ç¤ºæ–‡æœ¬å—å¤„ç†è¿›åº¦\n",
        "                self.progress.update_chunk_progress(i+1, len(chunks), file_path.name)\n",
        "\n",
        "                # ç”Ÿæˆé—®ç­”å¯¹\n",
        "                qa_pairs = self.qa_generator.generate_qa_pairs(chunk)\n",
        "\n",
        "                if qa_pairs:\n",
        "                    total_qa_pairs.extend(qa_pairs)\n",
        "\n",
        "                    # å¢é‡ä¿å­˜\n",
        "                    self.qa_generator.append_qa_pairs(filename, qa_pairs)\n",
        "\n",
        "                # é¿å…è¯·æ±‚è¿‡äºé¢‘ç¹\n",
        "                time.sleep(1)\n",
        "\n",
        "            # å®Œæˆæ–‡ä»¶å¤„ç†\n",
        "            self.progress.finish_file(file_path.name, len(total_qa_pairs))\n",
        "\n",
        "            # æœ€ç»ˆç»Ÿè®¡\n",
        "            logger.info(f\"æ–‡ä»¶ {file_path.name} å¤„ç†å®Œæˆ\")\n",
        "            logger.info(f\"ç”Ÿæˆé—®ç­”å¯¹æ€»æ•°: {len(total_qa_pairs)}\")\n",
        "\n",
        "            # self.progress.update_file(file_path.name, len(total_qa_pairs))  # å·²åœ¨ä¸Šé¢è°ƒç”¨finish_file\n",
        "            return True\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"å¤„ç†æ–‡ä»¶ {file_path} æ—¶å‡ºé”™: {str(e)}\")\n",
        "            self.progress.add_error()\n",
        "            return False\n",
        "\n",
        "    def process_all_files(self, chunk_size: int = 800, overlap_size: int = 200, resume: bool = True) -> Dict:\n",
        "        \"\"\"å¤„ç†æ‰€æœ‰æ–‡ä»¶\"\"\"\n",
        "        # è·å–æ–‡ä»¶åˆ—è¡¨\n",
        "        files = self.file_manager.get_file_list()\n",
        "        if not files:\n",
        "            logger.error(\"æœªæ‰¾åˆ°ä»»ä½•æ–‡æœ¬æ–‡ä»¶\")\n",
        "            return {\"success\": False, \"message\": \"æœªæ‰¾åˆ°æ–‡ä»¶\"}\n",
        "\n",
        "        # è¿‡æ»¤å·²å¤„ç†çš„æ–‡ä»¶ï¼ˆå¦‚æœå¯ç”¨ç»­ä¼ ï¼‰\n",
        "        if resume:\n",
        "            unprocessed_files = []\n",
        "            for file_path in files:\n",
        "                if not self.file_manager.check_existing_output(file_path.stem):\n",
        "                    unprocessed_files.append(file_path)\n",
        "                else:\n",
        "                    logger.info(f\"è·³è¿‡å·²å¤„ç†æ–‡ä»¶: {file_path.name}\")\n",
        "\n",
        "            files = unprocessed_files\n",
        "\n",
        "        if not files:\n",
        "            logger.info(\"æ‰€æœ‰æ–‡ä»¶éƒ½å·²å¤„ç†å®Œæˆ\")\n",
        "            return {\"success\": True, \"message\": \"æ‰€æœ‰æ–‡ä»¶å·²å¤„ç†\"}\n",
        "\n",
        "        # å¼€å§‹å¤„ç†\n",
        "        self.progress.start(len(files))\n",
        "        success_count = 0\n",
        "        failed_files = []\n",
        "\n",
        "        for file_path in files:\n",
        "            if self.process_single_file(file_path, chunk_size, overlap_size):\n",
        "                success_count += 1\n",
        "            else:\n",
        "                failed_files.append(file_path.name)\n",
        "\n",
        "        # æ˜¾ç¤ºå¤„ç†æ€»ç»“\n",
        "        self.progress.show_summary()\n",
        "\n",
        "        return {\n",
        "            \"success\": True,\n",
        "            \"processed\": success_count,\n",
        "            \"failed\": len(failed_files),\n",
        "            \"failed_files\": failed_files,\n",
        "            \"total_qa_pairs\": self.progress.stats.total_qa_pairs\n",
        "        }\n",
        "\n",
        "    def preview_file(self, file_path: Path, preview_chars: int = 500) -> Dict:\n",
        "        \"\"\"é¢„è§ˆæ–‡ä»¶å†…å®¹\"\"\"\n",
        "        try:\n",
        "            text = self.file_manager.load_text_file(file_path)\n",
        "            if not text:\n",
        "                return {\"success\": False, \"message\": \"æ— æ³•åŠ è½½æ–‡ä»¶\"}\n",
        "\n",
        "            # æ¸…ç†æ–‡æœ¬\n",
        "            cleaned_text = self.text_processor.clean_text(text)\n",
        "\n",
        "            # è·å–ç»Ÿè®¡ä¿¡æ¯\n",
        "            quality_stats = self.text_processor.validate_text_quality(cleaned_text)\n",
        "            doc_type = self.text_processor.detect_document_type(cleaned_text)\n",
        "\n",
        "            preview = cleaned_text[:preview_chars]\n",
        "            if len(cleaned_text) > preview_chars:\n",
        "                preview += \"...\"\n",
        "\n",
        "            return {\n",
        "                \"success\": True,\n",
        "                \"filename\": file_path.name,\n",
        "                \"doc_type\": doc_type,\n",
        "                \"total_length\": len(cleaned_text),\n",
        "                \"quality_score\": quality_stats['quality_score'],\n",
        "                \"preview\": preview\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            return {\"success\": False, \"message\": str(e)}\n",
        "\n",
        "# ä¾¿æ·å‡½æ•°\n",
        "def list_available_files():\n",
        "    \"\"\"åˆ—å‡ºå¯ç”¨çš„æ–‡ä»¶\"\"\"\n",
        "    # from part3 import FileManager\n",
        "\n",
        "    global FileManager\n",
        "    file_manager = FileManager()\n",
        "    files = file_manager.get_file_list()\n",
        "\n",
        "    if not files:\n",
        "        print(\"ğŸ“ processedç›®å½•ä¸­æœªæ‰¾åˆ°ä»»ä½•æ–‡æœ¬æ–‡ä»¶\")\n",
        "        print(\"è¯·ç¡®ä¿å·²å°†.txtæ–‡ä»¶æ”¾å…¥processedç›®å½•\")\n",
        "        return []\n",
        "\n",
        "    print(f\"ğŸ“ åœ¨processedç›®å½•ä¸­æ‰¾åˆ° {len(files)} ä¸ªæ–‡æœ¬æ–‡ä»¶:\")\n",
        "    total_size = 0\n",
        "    for i, file_path in enumerate(files, 1):\n",
        "        file_size = file_path.stat().st_size if file_path.exists() else 0\n",
        "        total_size += file_size\n",
        "        print(f\"  {i:2d}. {file_path.name:<30} ({file_size:,} bytes)\")\n",
        "\n",
        "    print(f\"ğŸ“Š æ€»è®¡: {len(files)} ä¸ªæ–‡ä»¶, {total_size:,} bytes\")\n",
        "    return files\n",
        "\n",
        "def process_single_file(filename: str, chunk_size: int = 800):\n",
        "    \"\"\"å¤„ç†å•ä¸ªæ–‡ä»¶çš„ä¾¿æ·å‡½æ•°\"\"\"\n",
        "    if not filename.endswith('.txt'):\n",
        "        filename += '.txt'\n",
        "\n",
        "    file_path = Path(\"./processed\") / filename\n",
        "    if not file_path.exists():\n",
        "        print(f\"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {file_path}\")\n",
        "        return False\n",
        "\n",
        "    print(f\"ğŸš€ å¼€å§‹å¤„ç†æ–‡ä»¶: {filename}\")\n",
        "    global GMPProcessor\n",
        "    processor = GMPProcessor()\n",
        "    return processor.process_single_file(file_path, chunk_size)\n",
        "\n",
        "def process_all_files(chunk_size: int = 800, resume: bool = True):\n",
        "    \"\"\"å¤„ç†æ‰€æœ‰æ–‡ä»¶çš„ä¾¿æ·å‡½æ•°\"\"\"\n",
        "    global GMPProcessor\n",
        "    processor = GMPProcessor()\n",
        "    return processor.process_all_files(chunk_size=chunk_size, resume=resume)\n",
        "\n",
        "def preview_file(filename: str):\n",
        "    \"\"\"é¢„è§ˆæ–‡ä»¶çš„ä¾¿æ·å‡½æ•°\"\"\"\n",
        "    if not filename.endswith('.txt'):\n",
        "        filename += '.txt'\n",
        "\n",
        "    file_path = Path(\"./processed\") / filename\n",
        "    if not file_path.exists():\n",
        "        print(f\"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {file_path}\")\n",
        "        return None\n",
        "\n",
        "    print(f\"ğŸ” é¢„è§ˆæ–‡ä»¶: {filename}\")\n",
        "    global GMPProcessor\n",
        "    processor = GMPProcessor()\n",
        "    return processor.preview_file(file_path)\n",
        "\n",
        "def show_results():\n",
        "    \"\"\"æ˜¾ç¤ºå¤„ç†ç»“æœ\"\"\"\n",
        "    json_dir = Path(\"./jsons\")\n",
        "    if not json_dir.exists():\n",
        "        print(\"âŒ jsonsç›®å½•ä¸å­˜åœ¨\")\n",
        "        print(\"è¯·ç¡®ä¿å·²åˆ›å»ºjsonsç›®å½•\")\n",
        "        return\n",
        "\n",
        "    json_files = list(json_dir.glob(\"*.json\"))\n",
        "    if not json_files:\n",
        "        print(\"ğŸ“‚ jsonsç›®å½•ä¸­æœªæ‰¾åˆ°å¤„ç†ç»“æœ\")\n",
        "        print(\"è¯·å…ˆè¿è¡Œå¤„ç†å‡½æ•°ç”Ÿæˆé—®ç­”å¯¹\")\n",
        "        return\n",
        "\n",
        "    total_qa_pairs = 0\n",
        "    total_size = 0\n",
        "\n",
        "    print(f\"\\nğŸ“Š å¤„ç†ç»“æœ ({len(json_files)} ä¸ªJSONæ–‡ä»¶):\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    for json_file in sorted(json_files):\n",
        "        try:\n",
        "            with open(json_file, 'r', encoding='utf-8') as f:\n",
        "                qa_pairs = json.load(f)\n",
        "                qa_count = len(qa_pairs)\n",
        "                total_qa_pairs += qa_count\n",
        "\n",
        "            file_size = json_file.stat().st_size\n",
        "            total_size += file_size\n",
        "\n",
        "            print(f\"  ğŸ“„ {json_file.stem:<30} {qa_count:4d} Q&A  ({file_size:,} bytes)\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"  âŒ {json_file.stem}: è¯»å–é”™è¯¯ - {e}\")\n",
        "\n",
        "    print(\"-\" * 60)\n",
        "    print(f\"ğŸ“ˆ æ€»è®¡: {len(json_files)} ä¸ªæ–‡ä»¶, {total_qa_pairs:,} ä¸ªé—®ç­”å¯¹, {total_size:,} bytes\")\n",
        "\n",
        "    # æ˜¾ç¤ºå¹³å‡ç»Ÿè®¡\n",
        "    if json_files:\n",
        "        avg_qa = total_qa_pairs / len(json_files)\n",
        "        print(f\"ğŸ“Š å¹³å‡: {avg_qa:.1f} é—®ç­”å¯¹/æ–‡ä»¶\")\n",
        "\n",
        "    print(f\"ğŸ“‚ ç»“æœä¿å­˜åœ¨: {json_dir}/\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "def download_results():\n",
        "    \"\"\"ä¸‹è½½ç»“æœæ–‡ä»¶ï¼ˆColabä¸­ä½¿ç”¨ï¼‰\"\"\"\n",
        "    try:\n",
        "        from google.colab import files\n",
        "\n",
        "        json_dir = Path(\"./jsons\")\n",
        "        json_files = list(json_dir.glob(\"*.json\"))\n",
        "\n",
        "        if not json_files:\n",
        "            print(\"âŒ æœªæ‰¾åˆ°ç»“æœæ–‡ä»¶\")\n",
        "            print(\"è¯·å…ˆè¿è¡Œå¤„ç†å‡½æ•°ç”Ÿæˆé—®ç­”å¯¹\")\n",
        "            return\n",
        "\n",
        "        print(f\"ğŸ“¦ å‡†å¤‡ä¸‹è½½ {len(json_files)} ä¸ªç»“æœæ–‡ä»¶...\")\n",
        "        for i, json_file in enumerate(json_files, 1):\n",
        "            print(f\"  {i}/{len(json_files)}: {json_file.name}\")\n",
        "            files.download(str(json_file))\n",
        "\n",
        "        print(\"âœ… ä¸‹è½½å®Œæˆï¼\")\n",
        "\n",
        "    except ImportError:\n",
        "        print(\"âš ï¸ æ­¤åŠŸèƒ½ä»…åœ¨Google Colabä¸­å¯ç”¨\")\n",
        "        print(\"åœ¨æœ¬åœ°ç¯å¢ƒä¸­ï¼Œè¯·ç›´æ¥è®¿é—® ./jsons/ ç›®å½•\")\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ ä¸‹è½½å¤±è´¥: {e}\")\n",
        "\n",
        "# ä½¿ç”¨ç¤ºä¾‹å’Œè¯´æ˜\n",
        "def show_examples():\n",
        "    \"\"\"æ˜¾ç¤ºä½¿ç”¨ç¤ºä¾‹\"\"\"\n",
        "    examples = \"\"\"\n",
        "ä½¿ç”¨ç¤ºä¾‹:\n",
        "\n",
        "# 1. åˆ—å‡ºå¯ç”¨æ–‡ä»¶\n",
        "files = list_available_files()\n",
        "\n",
        "# 2. é¢„è§ˆæ–‡ä»¶\n",
        "preview = preview_file(\"ç¤ºä¾‹æ–‡ä»¶å.txt\")\n",
        "print(preview)\n",
        "\n",
        "# 3. å¤„ç†å•ä¸ªæ–‡ä»¶\n",
        "success = process_single_file(\"ç¤ºä¾‹æ–‡ä»¶å.txt\")\n",
        "\n",
        "# 4. å¤„ç†æ‰€æœ‰æ–‡ä»¶\n",
        "result = process_all_files(chunk_size=800, resume=True)\n",
        "\n",
        "# 5. æŸ¥çœ‹ç»“æœ\n",
        "show_results()\n",
        "\n",
        "# 6. ä¸‹è½½ç»“æœï¼ˆä»…åœ¨Colabä¸­ï¼‰\n",
        "download_results()\n",
        "\"\"\"\n",
        "    print(examples)\n",
        "\n",
        "# ä¸»å‡½æ•°ï¼ˆç”¨äºç›´æ¥è¿è¡Œï¼‰\n",
        "def main():\n",
        "    \"\"\"ä¸»å‡½æ•° - äº¤äº’å¼å¤„ç†\"\"\"\n",
        "    print(\"=\" * 60)\n",
        "    print(\"GMPé—®ç­”å¯¹ç”Ÿæˆå™¨ - Colabäº¤äº’æ¨¡å¼\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # æ˜¾ç¤ºå¯ç”¨æ–‡ä»¶\n",
        "    files = list_available_files()\n",
        "    if not files:\n",
        "        return\n",
        "\n",
        "    print(\"\\nè¯·é€‰æ‹©æ“ä½œ:\")\n",
        "    print(\"1. é¢„è§ˆæ–‡ä»¶\")\n",
        "    print(\"2. å¤„ç†å•ä¸ªæ–‡ä»¶\")\n",
        "    print(\"3. å¤„ç†æ‰€æœ‰æ–‡ä»¶\")\n",
        "    print(\"4. æŸ¥çœ‹ç»“æœ\")\n",
        "    print(\"5. æ˜¾ç¤ºä½¿ç”¨ç¤ºä¾‹\")\n",
        "\n",
        "    try:\n",
        "        choice = input(\"\\nè¯·è¾“å…¥é€‰æ‹© (1-5): \").strip()\n",
        "\n",
        "        if choice == \"1\":\n",
        "            filename = input(\"è¯·è¾“å…¥æ–‡ä»¶å: \").strip()\n",
        "            preview = preview_file(filename)\n",
        "            if preview and preview.get(\"success\"):\n",
        "                print(f\"\\næ–‡æ¡£ç±»å‹: {preview['doc_type']}\")\n",
        "                print(f\"è´¨é‡è¯„åˆ†: {preview['quality_score']}/100\")\n",
        "                print(f\"é¢„è§ˆå†…å®¹:\\n{preview['preview']}\")\n",
        "\n",
        "        elif choice == \"2\":\n",
        "            filename = input(\"è¯·è¾“å…¥æ–‡ä»¶å: \").strip()\n",
        "            success = process_single_file(filename)\n",
        "            print(\"å¤„ç†å®Œæˆ\" if success else \"å¤„ç†å¤±è´¥\")\n",
        "\n",
        "        elif choice == \"3\":\n",
        "            chunk_size = input(\"è¯·è¾“å…¥æ–‡æœ¬å—å¤§å° (é»˜è®¤800): \").strip()\n",
        "            chunk_size = int(chunk_size) if chunk_size.isdigit() else 800\n",
        "            result = process_all_files(chunk_size=chunk_size) # This was the call that was interrupted\n",
        "            print(f\"å¤„ç†ç»“æœ: {result}\")\n",
        "\n",
        "        elif choice == \"4\":\n",
        "            show_results()\n",
        "\n",
        "        elif choice == \"5\":\n",
        "            show_examples()\n",
        "\n",
        "        else:\n",
        "            print(\"æ— æ•ˆé€‰æ‹©\")\n",
        "\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"\\n\\næ“ä½œå·²å–æ¶ˆ\")\n",
        "    except Exception as e:\n",
        "        print(f\"\\næ“ä½œå¤±è´¥: {e}\")\n",
        "\n",
        "# åˆå§‹åŒ–è¯´æ˜\n",
        "def initialize_colab():\n",
        "    \"\"\"åˆå§‹åŒ–Colabç¯å¢ƒ\"\"\"\n",
        "    # from part1 import initialize_environment\n",
        "\n",
        "    print(\"ğŸš€ åˆå§‹åŒ–GMPé—®ç­”å¯¹ç”Ÿæˆå™¨...\")\n",
        "    print(\"ğŸ“ è¾“å…¥ç›®å½•: ./processed/\")\n",
        "    print(\"ğŸ“ è¾“å‡ºç›®å½•: ./jsons/\")\n",
        "    success = True\n",
        "    if success:\n",
        "        print(\"\\nâœ“ ç¯å¢ƒåˆå§‹åŒ–å®Œæˆ\")\n",
        "        print(\"å¯ä»¥ä½¿ç”¨ä»¥ä¸‹å‡½æ•°å¼€å§‹å¤„ç†:\")\n",
        "        print(\"- list_available_files()  # åˆ—å‡ºæ–‡ä»¶\")\n",
        "        print(\"- preview_file(filename)  # é¢„è§ˆæ–‡ä»¶\")\n",
        "        print(\"- process_single_file(filename)  # å¤„ç†å•ä¸ªæ–‡ä»¶\")\n",
        "        print(\"- process_all_files()  # å¤„ç†æ‰€æœ‰æ–‡ä»¶\")\n",
        "        print(\"- show_results()  # æ˜¾ç¤ºç»“æœ\")\n",
        "        print(\"- show_examples()  # æ˜¾ç¤ºç¤ºä¾‹\")\n",
        "    return success\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    initialize_colab()\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fRQKKvex8bR3",
        "outputId": "28674192-0516-4a7c-d65b-de2f5fa9b6fe"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-11-27 11:05:36,244 - INFO - åœ¨ ./processed ä¸­æ‰¾åˆ° 14 ä¸ªæ–‡æœ¬æ–‡ä»¶\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸš€ åˆå§‹åŒ–GMPé—®ç­”å¯¹ç”Ÿæˆå™¨...\n",
            "ğŸ“ è¾“å…¥ç›®å½•: ./processed/\n",
            "ğŸ“ è¾“å‡ºç›®å½•: ./jsons/\n",
            "\n",
            "âœ“ ç¯å¢ƒåˆå§‹åŒ–å®Œæˆ\n",
            "å¯ä»¥ä½¿ç”¨ä»¥ä¸‹å‡½æ•°å¼€å§‹å¤„ç†:\n",
            "- list_available_files()  # åˆ—å‡ºæ–‡ä»¶\n",
            "- preview_file(filename)  # é¢„è§ˆæ–‡ä»¶\n",
            "- process_single_file(filename)  # å¤„ç†å•ä¸ªæ–‡ä»¶\n",
            "- process_all_files()  # å¤„ç†æ‰€æœ‰æ–‡ä»¶\n",
            "- show_results()  # æ˜¾ç¤ºç»“æœ\n",
            "- show_examples()  # æ˜¾ç¤ºç¤ºä¾‹\n",
            "============================================================\n",
            "GMPé—®ç­”å¯¹ç”Ÿæˆå™¨ - Colabäº¤äº’æ¨¡å¼\n",
            "============================================================\n",
            "ğŸ“ åœ¨processedç›®å½•ä¸­æ‰¾åˆ° 14 ä¸ªæ–‡æœ¬æ–‡ä»¶:\n",
            "   1. 20220701 PICS-GMPGDP ç›‘ç®¡ç¯å¢ƒä¸­æ•°æ®ç®¡ç†å’Œå¯é æ€§çš„è‰¯å¥½å®è·µ_cleaned.txt (267,681 bytes)\n",
            "   2. å·²ä¸Šå¸‚ä¸­è¯è¯å­¦å˜æ›´ç ”ç©¶æŠ€æœ¯æŒ‡å¯¼åŸåˆ™ï¼ˆè¯•è¡Œï¼‰ã€‹_cleaned.txt (38,360 bytes)\n",
            "   3. 2023editionGMPGuideOralSolidDosagetxt_cleaned.txt (654,271 bytes)\n",
            "   4. DrugGMPGuideSterileProducts_Part2_2023edition_cleaned.txt (741,351 bytes)\n",
            "   5. 2023editionGMPGuideAPItxt_cleaned.txt (1,234,771 bytes)\n",
            "   6. GMPGuide2023edition_QualityManagementSystem_W_cleaned.txt (860,452 bytes)\n",
            "   7. 202501æ¸…æ´éªŒè¯æŠ€æœ¯æŒ‡å—_cleaned.txt     (101,900 bytes)\n",
            "   8. pe-009-16-gmp-guide-annexes_cleaned.txt (576,142 bytes)\n",
            "   9. å·²ä¸Šå¸‚åŒ–å­¦è¯å“è¯å­¦å˜æ›´ç ”ç©¶æŠ€æœ¯æŒ‡å¯¼åŸåˆ™ï¼ˆè¯•è¡Œï¼‰_cleaned.txt (63,865 bytes)\n",
            "  10. 2023editionGMPGuideSterileProductsPart1txt_cleaned.txt (1,524,716 bytes)\n",
            "  11. 2023editionGMPGuideQualityControlLab20230508txt_cleaned.txt (701,751 bytes)\n",
            "  12. 202303-å›½å®¶å±€æ ¸æŸ¥ä¸­å¿ƒ-è¯å“å…±çº¿ç”Ÿäº§è´¨é‡é£é™©ç®¡ç†æŒ‡å—_cleaned.txt (69,587 bytes)\n",
            "  13. å…å’¨-EU GMP é™„å½•1ï¼šæ— èŒè¯å“ç”Ÿäº§ï¼ˆä¸­è‹±å¯¹ç…§ç‰ˆï¼‰-20220822_cleaned.txt (319,432 bytes)\n",
            "  14. 20220701 PICS-GMPGDP ç›‘ç®¡ç¯å¢ƒä¸­æ•°æ®ç®¡ç†å’Œå¯é æ€§çš„è‰¯å¥½å®è·µ_cleaned_cleaned.txt (266,288 bytes)\n",
            "ğŸ“Š æ€»è®¡: 14 ä¸ªæ–‡ä»¶, 7,420,567 bytes\n",
            "\n",
            "è¯·é€‰æ‹©æ“ä½œ:\n",
            "1. é¢„è§ˆæ–‡ä»¶\n",
            "2. å¤„ç†å•ä¸ªæ–‡ä»¶\n",
            "3. å¤„ç†æ‰€æœ‰æ–‡ä»¶\n",
            "4. æŸ¥çœ‹ç»“æœ\n",
            "5. æ˜¾ç¤ºä½¿ç”¨ç¤ºä¾‹\n",
            "\n",
            "è¯·è¾“å…¥é€‰æ‹© (1-5): 3\n",
            "è¯·è¾“å…¥æ–‡æœ¬å—å¤§å° (é»˜è®¤800): 400\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-11-27 11:06:06,623 - INFO - âœ“ Google Colab AIå®¢æˆ·ç«¯å·²åˆå§‹åŒ–\n",
            "2025-11-27 11:06:06,625 - INFO - åœ¨ ./processed ä¸­æ‰¾åˆ° 14 ä¸ªæ–‡æœ¬æ–‡ä»¶\n",
            "2025-11-27 11:06:06,627 - INFO - å¼€å§‹å¤„ç† 14 ä¸ªæ–‡ä»¶...\n",
            "2025-11-27 11:06:06,628 - INFO - å¼€å§‹å¤„ç†æ–‡ä»¶: 20220701 PICS-GMPGDP ç›‘ç®¡ç¯å¢ƒä¸­æ•°æ®ç®¡ç†å’Œå¯é æ€§çš„è‰¯å¥½å®è·µ_cleaned.txt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸš€ å¼€å§‹å¤„ç† 14 ä¸ªæ–‡ä»¶..."
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-11-27 11:06:06,866 - INFO - æ–‡æœ¬è´¨é‡è¯„åˆ†: 30/100\n",
            "2025-11-27 11:06:06,874 - INFO - æ–‡æ¡£ç±»å‹: GMP\n",
            "2025-11-27 11:06:06,878 - INFO - å¤„ç†åçš„æ–‡æœ¬å·²ä¿å­˜: processed/20220701 PICS-GMPGDP ç›‘ç®¡ç¯å¢ƒä¸­æ•°æ®ç®¡ç†å’Œå¯é æ€§çš„è‰¯å¥½å®è·µ_cleaned_cleaned.txt\n",
            "2025-11-27 11:06:06,991 - INFO - æ–‡æœ¬åˆ†å—å®Œæˆï¼Œå…± 1251 ä¸ªå—ï¼Œå¹³å‡é•¿åº¦: 354 å­—ç¬¦\n",
            "2025-11-27 11:06:15,784 - ERROR - Colab AIè°ƒç”¨å¤±è´¥: object str can't be used in 'await' expression\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rğŸ“ 20220701 PICS-GMPGDP ç›‘ç®¡ç¯å¢ƒä¸­æ•°æ®ç®¡ç†å’Œå¯é æ€§çš„è‰¯å¥½å®è·µ_cleaned.txt [â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘] 0% (2/1251å—) æ€»è¿›åº¦: 0.0% Q&A: 0"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-11-27 11:06:27,025 - ERROR - Colab AIè°ƒç”¨å¤±è´¥: object str can't be used in 'await' expression\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rğŸ“ 20220701 PICS-GMPGDP ç›‘ç®¡ç¯å¢ƒä¸­æ•°æ®ç®¡ç†å’Œå¯é æ€§çš„è‰¯å¥½å®è·µ_cleaned.txt [â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘] 0% (3/1251å—) æ€»è¿›åº¦: 0.0% Q&A: 0"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-11-27 11:06:37,464 - ERROR - Colab AIè°ƒç”¨å¤±è´¥: object str can't be used in 'await' expression\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rğŸ“ 20220701 PICS-GMPGDP ç›‘ç®¡ç¯å¢ƒä¸­æ•°æ®ç®¡ç†å’Œå¯é æ€§çš„è‰¯å¥½å®è·µ_cleaned.txt [â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘] 0% (4/1251å—) æ€»è¿›åº¦: 0.0% Q&A: 0"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-11-27 11:06:49,824 - ERROR - Colab AIè°ƒç”¨å¤±è´¥: object str can't be used in 'await' expression\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rğŸ“ 20220701 PICS-GMPGDP ç›‘ç®¡ç¯å¢ƒä¸­æ•°æ®ç®¡ç†å’Œå¯é æ€§çš„è‰¯å¥½å®è·µ_cleaned.txt [â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘] 0% (5/1251å—) æ€»è¿›åº¦: 0.0% Q&A: 0"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-11-27 11:07:01,169 - ERROR - Colab AIè°ƒç”¨å¤±è´¥: object str can't be used in 'await' expression\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rğŸ“ 20220701 PICS-GMPGDP ç›‘ç®¡ç¯å¢ƒä¸­æ•°æ®ç®¡ç†å’Œå¯é æ€§çš„è‰¯å¥½å®è·µ_cleaned.txt [â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘] 0% (6/1251å—) æ€»è¿›åº¦: 0.0% Q&A: 0"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-11-27 11:07:08,091 - ERROR - Colab AIè°ƒç”¨å¤±è´¥: object str can't be used in 'await' expression\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rğŸ“ 20220701 PICS-GMPGDP ç›‘ç®¡ç¯å¢ƒä¸­æ•°æ®ç®¡ç†å’Œå¯é æ€§çš„è‰¯å¥½å®è·µ_cleaned.txt [â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘] 1% (7/1251å—) æ€»è¿›åº¦: 0.0% Q&A: 0"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-11-27 11:07:16,341 - ERROR - Colab AIè°ƒç”¨å¤±è´¥: object str can't be used in 'await' expression\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rğŸ“ 20220701 PICS-GMPGDP ç›‘ç®¡ç¯å¢ƒä¸­æ•°æ®ç®¡ç†å’Œå¯é æ€§çš„è‰¯å¥½å®è·µ_cleaned.txt [â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘] 1% (8/1251å—) æ€»è¿›åº¦: 0.0% Q&A: 0"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-11-27 11:07:24,036 - ERROR - Colab AIè°ƒç”¨å¤±è´¥: object str can't be used in 'await' expression\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rğŸ“ 20220701 PICS-GMPGDP ç›‘ç®¡ç¯å¢ƒä¸­æ•°æ®ç®¡ç†å’Œå¯é æ€§çš„è‰¯å¥½å®è·µ_cleaned.txt [â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘] 1% (9/1251å—) æ€»è¿›åº¦: 0.0% Q&A: 0\n",
            "\n",
            "æ“ä½œå·²å–æ¶ˆ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "da1aca0f",
        "outputId": "5546826f-1657-4504-f802-b9f3dd748c89"
      },
      "source": [
        "process_all_files(chunk_size=400, resume=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-11-27 11:11:07,054 - INFO - âœ“ Google Colab AIå®¢æˆ·ç«¯å·²åˆå§‹åŒ–\n",
            "2025-11-27 11:11:07,059 - INFO - åœ¨ ./processed ä¸­æ‰¾åˆ° 15 ä¸ªæ–‡æœ¬æ–‡ä»¶\n",
            "2025-11-27 11:11:07,060 - INFO - å¼€å§‹å¤„ç† 15 ä¸ªæ–‡ä»¶...\n",
            "2025-11-27 11:11:07,063 - INFO - å¼€å§‹å¤„ç†æ–‡ä»¶: 20220701 PICS-GMPGDP ç›‘ç®¡ç¯å¢ƒä¸­æ•°æ®ç®¡ç†å’Œå¯é æ€§çš„è‰¯å¥½å®è·µ_cleaned.txt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸš€ å¼€å§‹å¤„ç† 15 ä¸ªæ–‡ä»¶..."
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-11-27 11:11:07,289 - INFO - æ–‡æœ¬è´¨é‡è¯„åˆ†: 30/100\n",
            "2025-11-27 11:11:07,296 - INFO - æ–‡æ¡£ç±»å‹: GMP\n",
            "2025-11-27 11:11:07,300 - INFO - å¤„ç†åçš„æ–‡æœ¬å·²ä¿å­˜: processed/20220701 PICS-GMPGDP ç›‘ç®¡ç¯å¢ƒä¸­æ•°æ®ç®¡ç†å’Œå¯é æ€§çš„è‰¯å¥½å®è·µ_cleaned_cleaned.txt\n",
            "2025-11-27 11:11:07,398 - INFO - æ–‡æœ¬åˆ†å—å®Œæˆï¼Œå…± 1251 ä¸ªå—ï¼Œå¹³å‡é•¿åº¦: 354 å­—ç¬¦\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ“ 20220701 PICS-GMPGDP ç›‘ç®¡ç¯å¢ƒä¸­æ•°æ®ç®¡ç†å’Œå¯é æ€§çš„è‰¯å¥½å®è·µ_cleaned.txt [â–“â–“â–“â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘] 26% (321/1251å—) æ€»è¿›åº¦: 0.0% Q&A: 0"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-11-27 11:51:24,052 - ERROR - Colab AIè°ƒç”¨å¤±è´¥: Error code: 429 - {'message': 'Insufficient quota available to perform this operation. Try again later.', 'type': 'invalid_request_error'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rğŸ“ 20220701 PICS-GMPGDP ç›‘ç®¡ç¯å¢ƒä¸­æ•°æ®ç®¡ç†å’Œå¯é æ€§çš„è‰¯å¥½å®è·µ_cleaned.txt [â–“â–“â–“â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘] 26% (322/1251å—) æ€»è¿›åº¦: 0.0% Q&A: 0"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-11-27 11:51:26,685 - ERROR - Colab AIè°ƒç”¨å¤±è´¥: Error code: 429 - {'message': 'Insufficient quota available to perform this operation. Try again later.', 'type': 'invalid_request_error'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rğŸ“ 20220701 PICS-GMPGDP ç›‘ç®¡ç¯å¢ƒä¸­æ•°æ®ç®¡ç†å’Œå¯é æ€§çš„è‰¯å¥½å®è·µ_cleaned.txt [â–“â–“â–“â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘] 26% (323/1251å—) æ€»è¿›åº¦: 0.0% Q&A: 0"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-11-27 11:51:29,155 - ERROR - Colab AIè°ƒç”¨å¤±è´¥: Error code: 429 - {'message': 'Insufficient quota available to perform this operation. Try again later.', 'type': 'invalid_request_error'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rğŸ“ 20220701 PICS-GMPGDP ç›‘ç®¡ç¯å¢ƒä¸­æ•°æ®ç®¡ç†å’Œå¯é æ€§çš„è‰¯å¥½å®è·µ_cleaned.txt [â–“â–“â–“â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘] 26% (324/1251å—) æ€»è¿›åº¦: 0.0% Q&A: 0"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-11-27 11:51:31,764 - ERROR - Colab AIè°ƒç”¨å¤±è´¥: Error code: 429 - {'message': 'Insufficient quota available to perform this operation. Try again later.', 'type': 'invalid_request_error'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rğŸ“ 20220701 PICS-GMPGDP ç›‘ç®¡ç¯å¢ƒä¸­æ•°æ®ç®¡ç†å’Œå¯é æ€§çš„è‰¯å¥½å®è·µ_cleaned.txt [â–“â–“â–“â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘] 26% (325/1251å—) æ€»è¿›åº¦: 0.0% Q&A: 0"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-11-27 11:51:34,299 - ERROR - Colab AIè°ƒç”¨å¤±è´¥: Error code: 429 - {'message': 'Insufficient quota available to perform this operation. Try again later.', 'type': 'invalid_request_error'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rğŸ“ 20220701 PICS-GMPGDP ç›‘ç®¡ç¯å¢ƒä¸­æ•°æ®ç®¡ç†å’Œå¯é æ€§çš„è‰¯å¥½å®è·µ_cleaned.txt [â–“â–“â–“â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘] 26% (326/1251å—) æ€»è¿›åº¦: 0.0% Q&A: 0"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-11-27 11:51:36,758 - ERROR - Colab AIè°ƒç”¨å¤±è´¥: Error code: 429 - {'message': 'Insufficient quota available to perform this operation. Try again later.', 'type': 'invalid_request_error'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rğŸ“ 20220701 PICS-GMPGDP ç›‘ç®¡ç¯å¢ƒä¸­æ•°æ®ç®¡ç†å’Œå¯é æ€§çš„è‰¯å¥½å®è·µ_cleaned.txt [â–“â–“â–“â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘] 26% (327/1251å—) æ€»è¿›åº¦: 0.0% Q&A: 0"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-11-27 11:51:39,596 - ERROR - Colab AIè°ƒç”¨å¤±è´¥: Error code: 429 - {'message': 'Insufficient quota available to perform this operation. Try again later.', 'type': 'invalid_request_error'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rğŸ“ 20220701 PICS-GMPGDP ç›‘ç®¡ç¯å¢ƒä¸­æ•°æ®ç®¡ç†å’Œå¯é æ€§çš„è‰¯å¥½å®è·µ_cleaned.txt [â–“â–“â–“â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘] 26% (328/1251å—) æ€»è¿›åº¦: 0.0% Q&A: 0"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-11-27 11:51:42,319 - ERROR - Colab AIè°ƒç”¨å¤±è´¥: Error code: 429 - {'message': 'Insufficient quota available to perform this operation. Try again later.', 'type': 'invalid_request_error'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rğŸ“ 20220701 PICS-GMPGDP ç›‘ç®¡ç¯å¢ƒä¸­æ•°æ®ç®¡ç†å’Œå¯é æ€§çš„è‰¯å¥½å®è·µ_cleaned.txt [â–“â–“â–“â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘] 26% (329/1251å—) æ€»è¿›åº¦: 0.0% Q&A: 0"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-11-27 11:51:44,772 - ERROR - Colab AIè°ƒç”¨å¤±è´¥: Error code: 429 - {'message': 'Insufficient quota available to perform this operation. Try again later.', 'type': 'invalid_request_error'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rğŸ“ 20220701 PICS-GMPGDP ç›‘ç®¡ç¯å¢ƒä¸­æ•°æ®ç®¡ç†å’Œå¯é æ€§çš„è‰¯å¥½å®è·µ_cleaned.txt [â–“â–“â–“â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘] 26% (330/1251å—) æ€»è¿›åº¦: 0.0% Q&A: 0"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-11-27 11:51:47,412 - ERROR - Colab AIè°ƒç”¨å¤±è´¥: Error code: 429 - {'message': 'Insufficient quota available to perform this operation. Try again later.', 'type': 'invalid_request_error'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rğŸ“ 20220701 PICS-GMPGDP ç›‘ç®¡ç¯å¢ƒä¸­æ•°æ®ç®¡ç†å’Œå¯é æ€§çš„è‰¯å¥½å®è·µ_cleaned.txt [â–“â–“â–“â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘] 26% (331/1251å—) æ€»è¿›åº¦: 0.0% Q&A: 0"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-11-27 11:51:50,290 - ERROR - Colab AIè°ƒç”¨å¤±è´¥: Error code: 429 - {'message': 'Insufficient quota available to perform this operation. Try again later.', 'type': 'invalid_request_error'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rğŸ“ 20220701 PICS-GMPGDP ç›‘ç®¡ç¯å¢ƒä¸­æ•°æ®ç®¡ç†å’Œå¯é æ€§çš„è‰¯å¥½å®è·µ_cleaned.txt [â–“â–“â–“â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘] 27% (332/1251å—) æ€»è¿›åº¦: 0.0% Q&A: 0"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-11-27 11:51:53,157 - ERROR - Colab AIè°ƒç”¨å¤±è´¥: Error code: 429 - {'message': 'Insufficient quota available to perform this operation. Try again later.', 'type': 'invalid_request_error'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rğŸ“ 20220701 PICS-GMPGDP ç›‘ç®¡ç¯å¢ƒä¸­æ•°æ®ç®¡ç†å’Œå¯é æ€§çš„è‰¯å¥½å®è·µ_cleaned.txt [â–“â–“â–“â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘] 27% (333/1251å—) æ€»è¿›åº¦: 0.0% Q&A: 0"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-11-27 11:51:55,672 - ERROR - Colab AIè°ƒç”¨å¤±è´¥: Error code: 429 - {'message': 'Insufficient quota available to perform this operation. Try again later.', 'type': 'invalid_request_error'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rğŸ“ 20220701 PICS-GMPGDP ç›‘ç®¡ç¯å¢ƒä¸­æ•°æ®ç®¡ç†å’Œå¯é æ€§çš„è‰¯å¥½å®è·µ_cleaned.txt [â–“â–“â–“â–“â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘] 27% (334/1251å—) æ€»è¿›åº¦: 0.0% Q&A: 0"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-11-27 11:51:58,409 - ERROR - Colab AIè°ƒç”¨å¤±è´¥: Error code: 429 - {'message': 'Insufficient quota available to perform this operation. Try again later.', 'type': 'invalid_request_error'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rğŸ“ 20220701 PICS-GMPGDP ç›‘ç®¡ç¯å¢ƒä¸­æ•°æ®ç®¡ç†å’Œå¯é æ€§çš„è‰¯å¥½å®è·µ_cleaned.txt [â–“â–“â–“â–“â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘] 27% (335/1251å—) æ€»è¿›åº¦: 0.0% Q&A: 0"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-11-27 11:52:00,989 - ERROR - Colab AIè°ƒç”¨å¤±è´¥: Error code: 429 - {'message': 'Insufficient quota available to perform this operation. Try again later.', 'type': 'invalid_request_error'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rğŸ“ 20220701 PICS-GMPGDP ç›‘ç®¡ç¯å¢ƒä¸­æ•°æ®ç®¡ç†å’Œå¯é æ€§çš„è‰¯å¥½å®è·µ_cleaned.txt [â–“â–“â–“â–“â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘] 27% (336/1251å—) æ€»è¿›åº¦: 0.0% Q&A: 0"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-11-27 11:52:03,699 - ERROR - Colab AIè°ƒç”¨å¤±è´¥: Error code: 429 - {'message': 'Insufficient quota available to perform this operation. Try again later.', 'type': 'invalid_request_error'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rğŸ“ 20220701 PICS-GMPGDP ç›‘ç®¡ç¯å¢ƒä¸­æ•°æ®ç®¡ç†å’Œå¯é æ€§çš„è‰¯å¥½å®è·µ_cleaned.txt [â–“â–“â–“â–“â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘] 27% (337/1251å—) æ€»è¿›åº¦: 0.0% Q&A: 0"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-11-27 11:52:06,324 - ERROR - Colab AIè°ƒç”¨å¤±è´¥: Error code: 429 - {'message': 'Insufficient quota available to perform this operation. Try again later.', 'type': 'invalid_request_error'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rğŸ“ 20220701 PICS-GMPGDP ç›‘ç®¡ç¯å¢ƒä¸­æ•°æ®ç®¡ç†å’Œå¯é æ€§çš„è‰¯å¥½å®è·µ_cleaned.txt [â–“â–“â–“â–“â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘] 27% (338/1251å—) æ€»è¿›åº¦: 0.0% Q&A: 0"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-11-27 11:52:09,101 - ERROR - Colab AIè°ƒç”¨å¤±è´¥: Error code: 429 - {'message': 'Insufficient quota available to perform this operation. Try again later.', 'type': 'invalid_request_error'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rğŸ“ 20220701 PICS-GMPGDP ç›‘ç®¡ç¯å¢ƒä¸­æ•°æ®ç®¡ç†å’Œå¯é æ€§çš„è‰¯å¥½å®è·µ_cleaned.txt [â–“â–“â–“â–“â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘] 27% (339/1251å—) æ€»è¿›åº¦: 0.0% Q&A: 0"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-11-27 11:52:11,825 - ERROR - Colab AIè°ƒç”¨å¤±è´¥: Error code: 429 - {'message': 'Insufficient quota available to perform this operation. Try again later.', 'type': 'invalid_request_error'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rğŸ“ 20220701 PICS-GMPGDP ç›‘ç®¡ç¯å¢ƒä¸­æ•°æ®ç®¡ç†å’Œå¯é æ€§çš„è‰¯å¥½å®è·µ_cleaned.txt [â–“â–“â–“â–“â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘] 27% (340/1251å—) æ€»è¿›åº¦: 0.0% Q&A: 0"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-11-27 11:52:14,614 - ERROR - Colab AIè°ƒç”¨å¤±è´¥: Error code: 429 - {'message': 'Insufficient quota available to perform this operation. Try again later.', 'type': 'invalid_request_error'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rğŸ“ 20220701 PICS-GMPGDP ç›‘ç®¡ç¯å¢ƒä¸­æ•°æ®ç®¡ç†å’Œå¯é æ€§çš„è‰¯å¥½å®è·µ_cleaned.txt [â–“â–“â–“â–“â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘] 27% (341/1251å—) æ€»è¿›åº¦: 0.0% Q&A: 0"
          ]
        }
      ]
    }
  ]
}